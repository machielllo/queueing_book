#+title: Sakasegawa's formula and Batching rules
#+author: Nicky D. van Foreest
#+date: 2022:01:19

#+STARTUP: indent
#+STARTUP: showall
#+PROPERTY: header-args:shell :exports both
#+PROPERTY: header-args:emacs-lisp :eval no-export
#+PROPERTY: header-args:python :eval no-export
# +PROPERTY: header-args:python :session  :exports both   :dir "./figures/" :results output

# +include: preamble.org

#+OPTIONS: toc:nil author:nil date:nil title:t

#+LATEX_CLASS: subfiles
#+LATEX_CLASS_OPTIONS: [assignments]

#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat "\\chapter{%t}\n"))
#+end_src


* TODO Set theme and font size for YouTube                         :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-operandi)
(set-face-attribute 'default nil :height 200)
#+end_src


* Intro

Here we study how to apply Sakasegawa's formula in various cases with and without batching, and check the quality of this approximation.

* Various models compared

We have (at least) three different types of models for a queueing system: simulation in  discrete time, simulation in  continuous,  and Sakasegawa's formula to compute the expected waiting time in queue. Let's see how these models compare.

** Discrete time simulation

We have a machine that can serve $c_k$ jobs on day $k$. When the period length is $T$, then $c_k\sim\Pois{\mu T}$. The number jobs that  arrive on $k$ is $a_k\sim\Pois{\lambda T}$. The arrivals in period $k$ cannot be served on day $k$. The code to simulate this should be familiar to you by now.

#+begin_src python
import numpy as np

np.random.seed(3)

labda = 3
mu = 1.2 * labda
T = 8  # period length, 8 hours in a day

num = 1000

a = np.random.poisson(labda * T, size=num)
c = np.random.poisson(mu * T, size=num)
L = np.zeros_like(a, dtype=int)

L[0] = 5
for i in range(1, num):
    d = min(c[i], L[i - 1])
    L[i] = L[i - 1] + a[i] - d


print(L.mean(), L.std())
#+end_src

#+begin_exercise
a. Why is the expected number of arrivals in a period equal to 24?
b. Run this code (and write down the results).
c. Then change the code such that  the arrivals can be served on the day they arrive. Rerun and compare the results to the case in which arrivals cannot be served on the day in which they arrive.
#+begin_hint
Replace the relevant line by ~d = min(c[i], L[i - 1] + a[i])~.
#+end_hint
#+end_exercise

#+begin_exercise
Now change the period time, which was 8 hours in a day, to $T=1$ (so that we concentrate on what happens during an hour instead of a day).

a. Why are now the expected number of arrivals during a period equal to $3$?
b. Rerun the code with and without the arrivals being served on the period of arrival.
c. compare to the previous exercise. You should notice that there is less difference in $\E L$ between the models in which you serve or don't serve jobs in the period they arrive. But why is that so?
#+end_exercise

** Continuous time simulation

Here is the code.

#+begin_src python
import numpy as np

np.random.seed(3)

labda = 3
mu = 1.2 * labda
num = 1000
X = np.random.exponential(scale=1 / labda, size=num)
A = np.zeros(len(X) + 1)
A[1:] = X.cumsum()
S = np.random.exponential(scale=1 / mu, size=len(A))
S[0] = 0
D = np.zeros_like(A)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

J = D - A
EL = labda * J.mean()
print(EL)
#+end_src

#+begin_exercise
Explain that we use Little's law to compute $\E L$. Can we use this law to estimate $\V L$?
#+end_exercise

#+begin_exercise
Run the code and compare with the discrete time simulation.
#+end_exercise

** Sakasegawa's formula

Now we use Sakasegawa's formula, rather than simulation, to compute $\E L$.

#+begin_src python
import numpy as np


def sakasegawa(labda, ES, Ca2=1, Cs2=1, c=1):
    rho = labda * ES / c
    V = (Ca2 + Cs2) / 2
    U = rho ** (np.sqrt(2 * (c + 1)) - 1) / (1 - rho)
    T = ES / c
    return V * U * T


labda = 3
mu = 1.2 * labda
ES = 1 / mu
W = sakasegawa(labda, 1 / mu, 1, 1, 1)
L = labda * (W + ES)
print(L)
#+end_src

#+begin_exercise
Run the code, and compare the value with the discrete and continuous time simulations.
#+end_exercise

You should notice that there is quite a difference between the estimates for $\E L$ we obtain from simulation and what we get from Sakasegawa's formula.
We conclude that it takes quite a bit of time for a simulation to reach `steady state'.


* Server setup and batching

We can now setup a model in which jobs arrive in batches of size $B$ and in between batches the server needs a constant setup time $R$. Check the queueing book for further background; we are going to build the model of the related section.

**  Sakasegawa's formula

I build up the code in small  blocks. You should put the code blocks underneath each other as you progress.

Since we add setup times, we should be carefull to avoid a situation which the  load is too large (recall, setup times add to the service times).

#+begin_src python
import numpy as np

np.random.seed(3)

B = 13
labda = 3
mu = 2 * labda
R = 2

rho = labda * (1 / mu + R / B)
assert rho < 1, f"{rho=} >= 1"
#+end_src


#+begin_exercise
a. What does the ~assert~ statement do?
b. Why do I put it here, i.e., before doing any other work?
c. What happens if you would set ~labda=10~ and ~mu=3~.
#+end_exercise

Next, we need to get the parameters correct for the batches. I just follow the book.

#+begin_src python
W1 = (B - 1) / 2 / labda
#+end_src

#+begin_exercise
What is ~W1~ conceptually:?
#+end_exercise

For the queueing time, we have this:
#+begin_src python
labdaB = labda / B
VR = 0  # constant R
ca2 = 1 / B
ES0 = 1 / mu
ES = ES0 + R / B
ESB = B * ES
VS0 = ES0 * ES0
VSB = VR + B * VS0
cs2 = VSB / ESB ** 2
#+end_src

#+begin_exercise
If  the interarrival times $X_k \sim \Exp{\lambda}$, why should we set ~ca2 = 1/B~?
#+end_exercise

For the average queueing time:
#+begin_src python
def sakasegawa(labda, ES, Ca2, Cs2, c=1):
    rho = labda * ES
    V = (Ca2 + Cs2) / 2
    U = rho ** (np.sqrt(2 * (c + 1)) - 1) / (1 - rho)
    T = ES / c
    return V * U * T

W2 = sakasegawa(labdaB, ESB, ca2, cs2)
#+end_src

#+begin_exercise
Why should we call the function ~sakasegawa~ with the following parameters  ~sakasegawa(labdaB, ESB, ca2, cs2)~?
#+end_exercise

The last step of the queueing process can be coded like this:
#+begin_src python
W3 = R + (B + 1) / 2 * ES0
#+end_src

#+begin_exercise
What is the meaning of ~W3~?
#+end_exercise

The sojourn time:

#+begin_src python
J = W1 + W2 + W3
print(J)
#+end_src

** Simulation

To set up the simulation requires a bit  fiddling with slicing. It took me a bit of time, and print statements, to get the details right. Here is the code, with the print statements so that you can figure out how it works.

#+begin_src python
num = 1000
num = B * (num // B)  # get multiple of B
X = np.random.exponential(scale=1 / labda, size=num)
A0 = np.zeros(len(X) + 1)
A0[1:] = X.cumsum()
A = np.zeros_like(A0)
for i in range(num // B):
    st = i * B + 1  # start
    fi = (i + 1) * B  # finish
    A[st : fi + 1] = A0[fi]


S0 = np.random.exponential(scale=1 / mu, size=len(A))
S0[0] = 0
S = S0.copy()
S[1::B] += R

D = np.zeros_like(A)
for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

J = D - A0
print(J.mean())
#+end_src

#+begin_exercise
Clearly, ~A0~ are the arrival times of the jobs at the system. Explain that ~A~ corresponds to the arrival times of the /batches/ at the queue, and that ~S~ are the service times including  setup times.
(Use print statements to understand how the slicing, i.e.,  notation like ~A[st : fi +1]~, works.)
#+end_exercise

#+begin_exercise
Why is ~D-A0~ the sojourn time, and not ~D - A~?
#+begin_hint
Recall that $A0$ are the arrival times at the system before batching while $A$ are the times the jobs move as a batch to the queue.
#+end_hint
#+end_exercise

#+begin_exercise
Run the code for ~num = 1000~ and compare the results of the formulas and the simulation. (Ensure that both models use the same data.) Then extend to ~num = 1_000_000~ and check again. What do you see, and conclude?
#+end_exercise


** Getting things  right

While making the above code, I made several (tens of) errors, so that the simulation and the formulas gave different results. Here are the steps that I followed to get things right. Only after one step was correct, I moved on to the next.
1. Check with $B=1$ and $R=0$, since $B = 1$ is the single job case.
2. Keep $B=1$, set $R=0.2$. I had to change $\mu$ so that still $\rho < 1$.
3. Set $B=2$, $R=0$. Compare ~ES~ (input for Sakasegawa's formula) to ~S.mean()~ (input for simulation).
4. In the previous step I did not get corresponding results for  ~num = 10000~. Changing it to 1 million helped.

After these four steps, the simulation and the model gave similar results.
However, from a higher level of abstraction, I am not quite happy about this.
It is not realistic to wait until we have seen a million or so arrivals in any practical setting. My personal way to deal with this situation is like this (but not all people agree on what to do though):
- Simple formulas are tremendously useful to get /insight/ into  the main drivers of the behavior of a system. In other words, there is not better way to get  /qualitative/ understanding than with simple formulas.
- The quantitative quality of a formula need to not be too good.
- Building a simulator is intellectually rewarding as it helps understand the /dynamics/ of a system.
- Building a simulator is difficult; it's easy to make mistakes, in the code, in the model, in the data\ldots
- Simulation depends on large quantities of data. It's very hard (next to impossible) to /understand/ the output.
- The simple formulas can be used to check the output of a simulator when applied to simple cases.
All in all, I think that simulation and theoretical models should go hand in hand, as they offer different type of insight, and have different strengths and weaknesses.


* Server Adjustments

Consider now a queueing system in which the server needs an adjustments with probability $p$ (see the section on server adjustments in the book). The repair times are assumed constant, at first, with mean $2$. I expect you now to be able to fill in Sakasegawa's formula. It remains to show you how to simulate this system.

#+begin_src python
import numpy as np

np.random.seed(3)

labda = 3
mu = 2 * labda
p = 1 / 20
num = 10000

X = np.random.exponential(scale=1 / labda, size=num)
A = np.zeros(len(X) + 1)
A[1:] = X.cumsum()
S = np.random.exponential(scale=1 / mu, size=len(A))
R = 2 * np.ones(len(S))                              # this
I = np.random.uniform(size=len(S)) <= p
D = np.zeros_like(S)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k] + R[k] * I[k]

W = D - A - S
print(W.mean())
#+end_src

#+begin_exercise
Explain how ~D~ is computed.
#+end_exercise

# To see how the approximation works, glue the next code below the previous code.
# #+begin_src python
# def sakasegawa(labda, ES, Ca2=1, Cs2=1, c=1):
#     rho = labda * ES
#     V = (Ca2 + Cs2) / 2
#     U = rho ** (np.sqrt(2 * (c + 1)) - 1) / (1 - rho)
#     T = ES / c
#     return V * U * T


# ES0 = 1 / mu
# VS0 = ES0 * ES0
# ER = R.mean()
# ES = ES0 + p * ER
# rho = labda * ES
# assert rho < 1, "rho >= 1"
# VR = R.var()
# VS = VS0 [+ p * VR + p * (1 - p) * ER * ER
# Cs2 = VS / ES / ES
# W = sakasegawa(labda, ES, 1, Cs2, 1)
# print(W)
# #+end_src


#+begin_exercise
To test the code I set at first  ~R = 0 * np.ones(len(A))~ in the line marked as ~this~. Why would I do this (to what simpler queueing system can I compare the results of this program?)
#+end_exercise

#+begin_exercise
Now run the code, with ~R~ as in the code (not set as 0 such as in the previous exercise).  Compare the answers. Then set ~num = 100000~, i.e., 10 times as large. What is the effect?
#+end_exercise

#+begin_exercise
Now set ~R = np.random.exponential(scale=2, size=len(A))~. What is the effect on $\E W$? In general, do you see that indeed $\E W$ increases with the variability of the adjustments?
#+end_exercise

# #+begin_exercise
# What is the model behind this code?
# #+begin_src python :eval no-export
# import numpy as np

# np.random.seed(3)

# labda = 3
# mu = 4
# N = 1000

# X = np.random.exponential(scale=1 / labda, size=N)
# A = np.zeros(len(X) + 1)
# A[1:] = X.cumsum()
# S = np.random.exponential(scale=1 / mu, size=len(A))
# R = np.random.uniform(0, 0.1, size=len(A))

# D = np.zeros_like(A)
# for k in range(1, len(A)):
#     D[k] = max(D[k - 1], A[k]) + S[k] + R[k]

# W = D - A - S
# print(W.mean(), W.std())
# #+end_src
# #+end_exercise



# #+begin_exercise
# In stead of
# #+begin_src python :eval no-export
# for k in range(1, len(A)):
#     D[k] = max(D[k - 1], A[k]) + S[k] + R[k] * I[k]
# #+end_src
# we could write
# #+begin_src python :eval no-export
# for k in range(1, len(A)):
#     D[k] = max(D[k - 1] + R[k] * I[k], A[k]) + S[k]
# #+end_src
# What modeling choice would this change reflect? Which of these two models makes the sojourn smaller?
# #+begin_hint
# What is the influence on the setup? Do we still require that the setup has to be done immediately before a service starts?
# #+end_hint
# #+end_exercise


* Server failures

This time we focus on a server that can fail; again check the queueing book for the formulas. Here we just implement them.

# ** Check work

# Again, first we need to check that our (implementation of the) formulas for $\E S$ and $\V S$ are correct.

# #+begin_src python
# import numpy as np
# from scipy.stats import expon

# np.random.seed(3)

# labda = 3
# mu = 2 * labda
# ES0 = 1 / mu
# labda_f = 2
# ER = 0.5
# num = 10000

# S0 = np.random.exponential(scale=ES0, size=num + 1)
# N = np.random.poisson(labda_f * ES0, len(S0))
# R = expon(scale=ER)
# S = np.zeros_like(S0)
# for i in range(len(S0)):
#     S[i] = S0[i] + R.rvs(N[i]).sum()

# A = 1 / (1 + labda_f * ER)
# ES = ES0 / A
# print(ES, S.mean(), S0.mean() + N.mean() * R.mean())

# C02 = 1
# Cs2 = C02 + 2 * A * (1 - A) * ER / ES
# print(Cs2, S.var() / (S.mean() ** 2))
# #+end_src

# #+RESULTS:
# : 0.3333333333333333 0.3353397655570448 0.3314315808219902
# : 1.75 1.8072807575060008

# #+begin_exercise
# Run this code, and check the result. Then chance ~num~ to 100000 to see that the estimate improves.
# #+end_exercise

# #+begin_exercise
# Explain how we compute ~S[i]~.
# #+end_exercise

# ** The simulations

#+begin_src python
import numpy as np
from scipy.stats import expon

np.random.seed(3)

labda = 2
mu = 6
ES0 = 1 / mu
labda_f = 2
ER = 0.5
num = 10000

X = np.random.exponential(scale=1 / labda, size=num)
X[0] = 0
A = X.cumsum()

S0 = np.random.exponential(scale=ES0, size=num)
N = np.random.poisson(labda_f * ES0, len(S0))
R = expon(scale=ER)
S = np.zeros_like(S0)
for i in range(len(S0)):
    S[i] = S0[i] + R.rvs(N[i]).sum()


D = np.zeros_like(A)
for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

W = D - A - S
print(W.mean())
#+end_src


#+begin_exercise
Run the code and include the results in your assignment. Where do we include the break downs?
#+end_exercise

#+begin_exercise
Suppose you can choose between two alternative ways to improve the system.
Increase $\lambda_f$, and decrease $\E R$, but such that $\lambda_f \E R$ remains constant; or the other way around, decrease $\lambda_f$ and increase $\E R$. Which alternative has better influence on $\E W$?
#+end_exercise



* A simple tandem network

We have two queues in tandem. Here is the code to simulate this.

#+begin_src python
import numpy as np

np.random.seed(4)

labda = 3
mu1 = 1.2 * labda
num = 100000
X = np.random.exponential(scale=1 / labda, size=num)
A1 = np.zeros(len(X) + 1)
A1[1:] = X.cumsum()
S1 = np.random.exponential(scale=1 / mu1, size=len(A1))
D1 = np.zeros_like(A1)

for k in range(1, len(A1)):
    D1[k] = max(D1[k - 1], A1[k]) + S1[k]

W1 = D1 - A1 - S1

# queue two
A2 = D1
mu2 = 1.1 * labda
S2 = np.random.exponential(scale=1 / mu2, size=len(A2))
D2 = np.zeros_like(A2)

for k in range(1, len(A2)):
    D2[k] = max(D2[k - 1], A2[k]) + S2[k]

W2 = D2 - A2 - S2

J = D2 - A1
print(W1.mean(), S1.mean(), W2.mean(), S2.mean(), J.mean())
#+end_src

#+begin_exercise
Why is ~J = D2 - A1~?
#+end_exercise
