#+title: Simulation of queues in discrete time
#+author: Nicky D. van Foreest
#+date: 2022:01:19

#+STARTUP: indent
#+STARTUP: overview
#+PROPERTY: header-args:shell :exports both
#+PROPERTY: header-args:emacs-lisp :eval no-export
#+PROPERTY: header-args:python :eval no-export
# +PROPERTY: header-args:python :session  :exports both   :dir "./figures/" :results output


#+OPTIONS: toc:nil author:nil date:nil title:t

#+LATEX_CLASS: subfiles
#+LATEX_CLASS_OPTIONS: [assignments]

#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat "\\chapter{%t}\n"))
#+end_src


* TODO Set theme and font size for YouTube                         :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-operandi)
(set-face-attribute 'default nil :height 200)
#+end_src

#+RESULTS:

* Intro

Here I show how to set up an  environment in python to simulate queueing systems in discrete time.

Perhaps you find the following youtube movies helpful.
- https://youtu.be/DfYxayoQmjYc
- https://youtu.be/D8BIAoBICnw
- https://youtu.be/_BoagRyH5c0


* Deterministic queues
Here we consider a simple queueing system in which all is deterministic.

** one  period,  demand, service capacity, and queue

There is one server, jobs enter a queue in front of the server, and the server serves batches of customers, every hour say.
#+begin_src python
L = 10
a = 5
d = 8
L = L + a -d
print(L)
#+end_src

#+RESULTS:
: 7

#+begin_src python
L = 3
a = 5
c = 7
d = min(c, L)
L += a -d
print(d, L)
#+end_src

#+RESULTS:
| 3 | 5 |

** two periods

#+begin_src python
L = 3
a = 5
c = 7
d = min(c, L)
L += a - d

a = 6
d = min(c, L)
L += a - d
print(d, L)
#+end_src

#+RESULTS:
| 5 | 6 |

#+begin_exercise
Add a third period, and report your result in the assignment.
#+end_exercise


** many periods, use vectors and for loops
#+begin_src python
num = 5

a = 9*np.ones(num)
c = 10*np.ones(num)
L = np.zeros_like(a)

L[0] = 20
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d

print(L)
#+end_src

#+RESULTS:
| 20 | 19 | 18 | 17 | 16 |

#+begin_exercise
Run the code for 10 periods and report your result.
#+end_exercise

#+begin_exercise
What will be the queue after 100 or so periods?
#+end_exercise

* Stochastic/random behavior

Real queueing systems show lots of stochasticity: in the number of jobs that arrive in a period, and the time it takes to serve these jobs. In this section we extend the previous models so that we can  deal with randomness.

** simulate demand

#+begin_src python
import numpy as np

a = np.random.randint(5, 8, size=5)
print(a)
#+end_src

#+RESULTS:
: [6 6 7 6 6]

** Set seed


#+begin_src python
import numpy as np

np.random.seed(3)

a = np.random.randint(5, 8, size=5)
print(a)
#+end_src

#+begin_exercise
Why do we set the seed?
#+end_exercise



** Compute mean and std of simulated queue length for \rho \approx 1

We discuss the concept of load more formally in the course at a later point in time.
Conceptually the load is the rate at which work arrives. For instance, if $\lambda=5$ jobs arrive per hour, and each requires 20 minutes of work (on average), then we say that the load is $5\times 20/60 = 5/3$. Since one server can do only 1 hour of work per hour, we need at least two servers to deal with this load. We define the utilization $\rho$ as the load divided by the number of servers present.

In discrete time, we define $\rho$ as the average number of jobs arriving per period divided by the average number of jobs we can serve per period. Slightly more formally, for discrete time,
\begin{align}
\rho \approx \frac{\sum_{k=1}^{n} a_{k}}{\sum_{k=1}^n c_{k}}.
\end{align}
And formally, we should take the limit $n\to\infty$ (but such limits are  a bit hard to obtain in simulation).

#+begin_src python
import numpy as np

np.random.seed(3)
num = 5000

a = np.random.randint(5, 10, size=num)
c = 7 * np.ones(num)
L = np.zeros_like(a)

L[0] = 20
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d

print(a.mean(), c.mean(), L.mean(), L.std())
#+end_src

#+RESULTS:
: 6.4988 6.5 7.18626 1.5343425994216546

#+begin_exercise
Read the numpy documentation on =numpy.random.randint= to explain the range of random numbers that are given by this function. In view of that, what should =a.mean()= approximately be? Is that larger, equal or smaller than =c=?
#+end_exercise

** plot the queue length process

Glue the next code after the other code.

#+begin_src python
import matplotlib.pyplot as plt

plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete_1.pdf')
#+end_src

#+begin_src python :results value file :exports results
'queue-discrete_1.pdf'
#+end_src

#+begin_exercise
Comment on the graph you get. Did you expect such large excursions of the queue length process?
#+begin_hint
What is the average number of arrivals per period? What is the average number of jobs that can be served, i.e., the average service capacity? Are they close or not?
#+end_hint

#+end_exercise


** A trap:  integers versus floats

Suppose that we change the arrival rate a bit.


#+begin_src python
num = 5000

np.random.seed(3)
a = np.random.randint(5, 9, size=num)
c = (5+8)/2 * np.ones(num)
L = np.zeros_like(a)

L[0] = 20
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d


plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete-1-1.pdf')
#+end_src

Don't forget that the ~numpy~ library has to be imported (as we did before).

#+begin_exercise
Compare the definition of =a= and =c= to what we had earlier.
What is =a.mean()= now approximately?  Observe that the mean of =c= is now around $6.5$.
#+end_exercise

When you make the plot you should see that is is very different from the one before. To explain why this is so, the following somewhat cryptic question will be helpful, hopefully.

#+begin_exercise
What is $9-6.5$? What is =int(9-6.5)=, that is, run the code in python, and  type in the answer in your answer sheet. Explain the difference between these two numbers? Explain that since =L= stores /integers/, not floats, we actually use a service capacity of $7$, /not/ of $6.5$.
#+end_exercise

#+begin_exercise
The code below repairs the above problem. Explain which line is different from the previous code. How did that change repair the problem? Now explain the title of this section. (BTW, I made this type of error with floats and ints many, many  times. The reason to include these exercises is to make you aware of the problem, so that you can spot it when you make the same error.)

Include the graph in your report and explain the differences.
#+end_exercise

#+begin_src python
num = 5000

np.random.seed(3)
a = np.random.randint(5, 9, size=num)
c = (5+8)/2 * np.ones(num)
L = np.zeros_like(a, dtype=float)

L[0] = 20
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d


plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete-1-2.pdf')
#+end_src

** Serve arrivals in the same period as they arrive

#+begin_exercise
Change the code such that the arrivals that occur in period $i$ can also be served in period $i$. Include your code in your assignment. Then  make a graph (include that too of course) and compare your results with the results of the  simulation we do here (recall, in the simulations up to now, arrivals cannot be served in the periods in which they arrive).
#+end_exercise


** Drift when \rho > 1

#+begin_src python
num = 5_000

np.random.seed(3)
a = np.random.randint(5, 9, size=num)
c = (5+8)/2.3 * np.ones(num)
L = np.zeros_like(a, dtype=float)

L[0] = 20
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d


plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete_2.pdf')
#+end_src

#+begin_src python :results value file :exports results
'queue-discrete_2.pdf'
#+end_src


#+begin_exercise
Include the graph in your report. What is =c.mean()-a.mean()=? Explain the slope of the line (fitted  through the points.) (Check the hint.)
#+begin_hint
As a simple analogous problem: imagine you have bucket containing  10 liters. Water flows in from a hose at rate 3 liters per minute, but it flows out via another hose at rate 5 l/m. What is the net outflow? Why does it take 5 minutes before the bucket is empty?
#+end_hint

#+end_exercise

** Drift when \rho< 1, start with large queue.


#+begin_src python
num = 5_000

np.random.seed(3)
a = np.random.randint(5, 9, size=num)
c = (5+8)/1.8 * np.ones(num)
L = np.zeros_like(a, dtype=float)

L[0] = 2_000
for i in range(1, num):
    d = min(c[i], L[i-1])
    L[i] = L[i-1] + a[i] - d


plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete_3.pdf')
#+end_src

#+begin_src python :results value file :exports results
'queue-discrete_3.pdf'
#+end_src

#+begin_exercise
Explain the slope of the lines (if you would fit that through the points.)
#+end_exercise


** Hitting times

When $\rho < 1$ and $L_0$ is some large number we could be interested in estimating the time until the queue is empty. With this we can decide if extra capacity hired to remove the long queue (for instance, long waiting times in a hospital) suffices. If the time to hit zero is still too long, we should hire more capacity.

Here we study how to estimate the hitting time $\tau = \min \{k : Z_k \leq 0\}$, where $Z_k = Z_{k-1}+a_k-c_{k}$. (Observe that $Z$ and $L$ are not the same everywhere: $Z$ can become negative, the number $L$ in the system $L$ is always $\geq 0$.)

#+begin_exercise
Run the following code and include the figure in your report. Use the CTL (see the hint if you forgot) to explain that most sample paths of $Z$ seem to hit $0$ when
\begin{align*}
\tau \approx \frac{100}{(5+9)/1.9 - 7}.
\end{align*}
Next, change the factor 1.9 to 1.2. Why do the graphs lie much nearer to each other?

#+begin_src python
import numpy as np
import matplotlib.pyplot as plt

num = 500
L0 = 100


def hitting_time():
    a = np.random.randint(5, 10, size=num)
    c = (5 + 9) / 1.9 * np.ones(num)
    a[0] = L0
    Z = (a - c).cumsum()
    plt.plot(Z)
    return


samples = 30
for n in range(samples):
    hitting_time()

plt.savefig("figures/free-random-walk.pdf")
#+end_src
\begin{hint}
See the queueing book exercises 2.1.8 and 3.2.3 for further explanations.

When $L_{0} \gg 1$, then  $a_{k}$ jobs arrive in period $k$ and $c_{k}$ jobs leave.
For ease, write $h_{k} = c_k-a_k$.
Then the time $\tau$ to hit 0, i.e., the time until the queue is empty, is the smallest $\tau$ such that $\sum_{k=1}^{\tau} h_{k} \geq L_{0}$.
But then, by Wald's theorem: $\E \tau \E{h_{k}} = L_{0}$. What is $\E \tau$?

Moreover, consider any sum of random variables, then with  $\mu = \E X$, $\sigma$  the std of $X$, and $N$ the normal distribution, and by the central limit theorem,
\begin{equation}
\label{eq:1}
\frac{1}{n} \sum_{i=1}^{n} X_{i} \sim N(\mu, \frac{\sigma^{2}}{n}) \implies \sum_{i=1}^n X_i \sim N(n \mu, n\sigma^{2}).
\end{equation}
\end{hint}
#+end_exercise

We see in the figure of the previous exercise that there is considerable variation in the time the random walk hits zero---a bit more specifically, hits the set \(\{\ldots, -2, -1, 0\}\)---when $\rho$ is not very small. We need some code to compute $\tau$ for a sample path of $Z$.


#+begin_exercise
Explain how the loop over =i= in the function =hitting_time= computes $\tau$ for a specific sample path. You don't have to run the code, the aim is that you understand how the algorithm works.

#+begin_src python
import numpy as np

np.random.seed(3)

num = 500
L0 = 100


def hitting_time():
    a = np.random.randint(5, 10, size=num)
    c = (5 + 9) / 1.9 * np.ones(num)
    L = L0
    for i in range(1, num):
        L += +a[i] - c[i]
        if L <= 0:
            return i


samples = 10
tau = np.zeros(samples)
for n in range(samples):
    tau[n] = hitting_time()

print(tau.mean(), tau.std())
#+end_src
#+end_exercise


Once again, python (and R) are rather slow in comparison to C or fortran; the factor in speed can be 100 or more in the execution of for loops. For this reason I prefer to tinker a bit with code to push as much of the computation to numpy.

#+begin_exercise
Run this code, and explain the output of each print statement of this piece of code. In particular,
realize that for =np.argmax=: `In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.' (I found this explanation in the numpy documentation.)
#+begin_src python
import numpy as np

np.random.seed(3)

num = 10
L0 = 10
samples = 3


dims = (samples, num)
a = np.random.randint(5, 10, size=dims)
c = 8 * np.ones(dims)
a[:, 0] = L0
Z = (a - c).cumsum(axis=1)
print(Z)
print(Z <= 0)
print(np.argmax(Z <= 0, axis=1))
#+end_src
BTW, observe that I use small samples  to print the output, so that it's easy to see how everything works.
#+end_exercise

Here is the final code.

#+begin_src python
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

np.random.seed(3)

num = 400
L0 = 100
samples = 3000


dims = (samples, num)
a = np.random.randint(5, 10, size=dims)
c = (5 + 9) / 1.9 * np.ones(dims)
a[:, 0] = L0
Z = (a - c).cumsum(axis=1)
tau = np.argmax(Z <= 0, axis=1)
print(tau.mean(), tau.std())

tau_scaled = (tau - tau.mean()) / tau.std()
print(tau_scaled.mean(), tau_scaled.std())
bins = np.linspace(-3, 3, 40)

B = (bins[1:] + bins[:-1]) / 2

plt.hist(tau_scaled, bins=bins, density=True)
plt.plot(B, norm.pdf(B))
plt.savefig("figures/tau.pdf")
#+end_src


#+begin_exercise
Use the CLT to provide some intuition to see why =tau_scaled= is approximately normally distributed. Include the plot in your report.

Here are some specific points of interest in the code:
1. ~num=400~ just to guess to ensure that ~Z[400] < 0~ for all sample paths. This trick allowed me to use numpy. Otherwise I would have needed a for loop in python, which I wanted to avoid.
2.  =bins= contains the edges of the bins. To plot the pdf of the standard normal distribution, I need the mid points of the bins. This explains ~B~.
#+end_exercise



** Things to memorize

1. If the capacity is equal or less than the arrival rate, the queue length will explode.
2. If the capacity is larger than the arrival rate, the queue length will `stay around 0', roughly speaking.
3. If we start with a huge queue, but the service capacity is larger than the  arrival rate, then the queue will drain like a straight line (roughly).

* Queues with blocking


Consider  a queue that is subject to blocking: this means that
when the queue exceeds $K$, say, then the excess jobs are rejected.

** A simple rejection rule

#+begin_src python
num = 500

np.random.seed(3)
a = np.random.randint(0, 20, size=num)
c = 10*np.ones(num)
L = np.zeros_like(a)
loss = np.zeros_like(a)

K = 30 # max people in queue, otherwise they leave

L[0] = 28
for i in range(1, num):
    d = min(c[i], L[i-1])
    loss[i] = max(L[i-1] + a[i] - d - K, 0)  # this
    L[i] = L[i-1] + a[i] - d - loss[i] # this 2

lost_fraction = sum(loss)/sum(a)
print(lost_fraction)
#+end_src

#+RESULTS:
: 0.04359580654874076

#+BEGIN_exercise
Explain how the line marked as ~this~ works, in other words, how does that line implement a queue with loss?  In line ~this 2~ we subtract ~loss[i]~; why?
#+END_exercise

#+BEGIN_exercise
Why is, in this case,  ~dtype=float~ not necessary in the definition of ~L~?
#+END_exercise


#+begin_exercise
Add the following code to make  the graph of the (simulated) queue length process.
#+begin_src python
plt.clf()
plt.plot(L)
plt.savefig('figures/queue-discrete-loss.pdf')
#+end_src
Include the graph in your report. Does the blocking rule work as it should?
#+end_exercise

** Rejection at the start of the period

If we would assume that rejection occurs as the start of the period,  the code has to be as follows:

#+begin_src python  :exports code
for i in range(1, num):
    d = min(c[i], L[i-1])
    loss[i] = max(L[i-1] + a[i] - K, 0)
    L[i] = L[i-1] + a[i] - d - loss[i]

lost_fraction = sum(loss)/sum(a)
print(lost_fraction)
#+end_src

#+begin_exercise
Explain the line in the code that changed.
#+end_exercise


#+begin_exercise
Explain that this rule has the same effect as assuming that departures occur after the rejection.
#+end_exercise

** Estimating rejection probabilities

With the code below we can  estimate the distribution $p_{i} = \P{L=i}$.


#+begin_src python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(3)

num = 5000

np.random.seed(3)
a = np.random.randint(0, 18, size=num)
c = 10 * np.ones(num)
L = np.zeros_like(a)

K = 30
p = np.zeros(K + 1)

L[0] = 28
for i in range(1, num):
    d = min(c[i], L[i - 1])
    L[i] = min(L[i - 1] + a[i] - d, K)
    p[L[i]] += 1

plt.clf()
plt.plot(p)
plt.savefig('figures/queue-discrete-loss-p.pdf')
#+end_src

#+RESULTS:

#+begin_exercise
Why should =p= have a length of =K+1=? Then explain  how the code estimates $p_{i}$.
#+end_exercise

#+begin_exercise
Note that =p= is not normalized (i.e., sums up to 1). The following code repairs that. Explain how it works.
\begin{minted}{python}
p /= p.sum()
\end{minted}
#+end_exercise

** Rejection probabilities under high loads

#+begin_exercise
Change the parameters such $\rho=1.51$. Then make again a plot of $p$. (Just copy my code, but change the parameters or the distribution of ~a~, or ~c~.)
#+end_exercise

#+begin_exercise
Now change the parameters such  $\rho \approx 10$, use a simple argument to show that $\pi_{K-2}$ should be really small. Check this intuition by doing a simulation with the appropriate parameters. Include your code and the graph of ~p~.
#+end_exercise



* TODO Restore my emacs settings                                   :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 100)
#+end_src

#+RESULTS:
