#+title: Simulation of Queueing processes in continuous time
#+author: Nicky D. van Foreest
#+date: 2022:01:19

#+STARTUP: indent
#+STARTUP: showall
#+PROPERTY: header-args:python :eval no-export
# +PROPERTY: header-args:python :session  :exports both   :dir "./figures/" :results output


#+OPTIONS: toc:nil author:nil date:nil title:t

#+LATEX_CLASS: subfiles
#+LATEX_CLASS_OPTIONS: [assignments]

#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat "\\chapter{%t}\n"))
#+end_src



* TODO Set theme and font size for YouTube                         :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-operandi)
(set-face-attribute 'default nil :height 200)
#+end_src

#+begin_src shell :results none
mv queues-simulations-in-continuous-times.pdf ../
#+end_src

* Intro

I discuss two elegant algorithms to simulate the waiting time process. One is for a system with one server, and jobs are served in the order in which they arrive. The second is for a multi-server FIFO queue.

- YouTube: https://youtu.be/h1OTvdLs9ik


* Computing waiting times

Here we just follow the steps of the queueing book to construct a single server FIFO queue in continuous time and compute the waiting and sojourn times.


** Load standard modules

We need the standard libraries for numerical work and plotting.

#+begin_src python :exports code :results none
import numpy as np
import matplotlib.pylab as plt
from matplotlib import style

np.random.seed(3)
#+end_src

** Inter-arrival times

Simulate random interarrival times that are $\sim \Exp{\lambda}$, with $\lambda=3$. First I take just three jobs, so that I can print out all intermediate results and check how things work. Once I am convinced about the correctness, I run a simulation for many jobs.

#+begin_src python
num = 3
labda = 3
X = np.random.exponential(scale=labda, size=num)
print(X)
#+end_src

#+RESULTS:
: None


Here is an important check (I always forget the meaning of $\lambda$ when I provide it to the simulator)

#+begin_src python
num = 100
labda = 3
X = np.random.exponential(scale=labda, size=num)
print(X.mean())
#+end_src

#+begin_exercise
Explain that ~scale=labda~ sets the interarrival times to $3$, but that in our queueing models, $\lambda$ should correspond to the arrival rate. Why is the code below in line with what we want?

#+begin_src python
num = 3
labda = 3
X = np.random.exponential(scale=1/labda, size=num)
#+end_src
#+end_exercise



** Arrival times


\begin{exercise}
Why do we generate first random inter-arrival times, and use these to compute the arrival times? Why not directly generate random arrival times?
\end{exercise}

#+begin_src python
A = X.cumsum()
print(A)
#+end_src

#+RESULTS:
| 2.40084716 | 6.0953707 | 7.12666691 | 9.27178782 | 15.97508026 | 22.77363983 | 23.17624146 | 23.8729566 | 24.03147334 | 25.77527041 |


Check the output to see that the arrival time of job 0 is $A_0 > 0$. But I want time to start at time $A_0=0$.  Here is the trick to achieve that.

#+begin_src python
X[0] = 0
A = X.cumsum()
print(A)
#+end_src


#+begin_exercise
1. Why is this better?
2. Why can we remove  ~X[0]~ without fundementally changing the analysis?
#+begin_hint
The $X_{k}$ are iid.
#+end_hint
#+end_exercise


** Service times

We have arrival times. We next need the service times of the jobs. Assume they are $\sim\Exp{\mu}$ with $\mu$ somewhat larger than $\lambda$. (Recall this means that jobs can be served faster than that they arrive.)

#+begin_src python
mu = 1.2 * labda
S = np.random.exponential(scale=1/mu,size=len(A))
S[0] = 0
print(S)
#+end_src

Note, ~S[0]~ remains unused; it should correspond to job 0, but we neglect this  job 0 in the remainder.

#+begin_exercise
Why do I use ~size=len(A)~ in the definition of ~S~?
#+begin_hint
If I would not do this, and I would want to change the simulation length (the number of jobs), at how many places should I change this number?
#+end_hint
#+end_exercise

#+begin_exercise
Why do we set ~scale=1/mu~?
#+end_exercise

#+begin_exercise
It's easy to compute the mean service time like this
#+begin_src python :eval no-export
print(S.mean())
#+end_src
Explain that this is not exactly equal to $\E S$.
#+begin_hint
Did we really serve job 0? If ~num~ is big number, does it matter that we set ~S[0]=0~?
#+end_hint
#+end_exercise


** Departure times

The standard recursion to compute the departure times.

#+begin_src python
D = np.zeros_like(A)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

print(D)
#+end_src


** Sojourn times

How long do you stay in the system if you arrive at some time $A_n$ and you depart at $D_n$?

#+begin_src python
J = D - A
print(J)
#+end_src


** Waiting times

If your sojourn time is 10, say, and your service time at the server is 3 (and there is just one server and the service discipline is FIFO), then what was your time in queue?

#+begin_src python
W = J - S
print(W)
#+end_src

#+begin_exercise
Recall that we set ~S[0] = 0~. Suppose that we wouldn't have done this, and we would run the simulation for a small number of cases, why can ~W.mean()~ be negative?
#+begin_hint
We subtract ~S[0]~ as if we served the corresponding job, but did we actually serve it?
#+end_hint
#+end_exercise


* KPIs and plotting

The next step is to see how to compute the most important indicators to assess the performance of the system. We can use these, so-called, Key Performance Indicators (KPIs) to see whether we should add, for instance, service capacity.

** Relevant averages

#+begin_src python
print(W.mean(), W.std())
#+end_src

#+begin_src python
print(J.mean(), J.std())
#+end_src

#+begin_exercise
The mean of $W$ is not entirely correct in the way we compute it here. What is (just a bit) wrong?
#+begin_hint
We include ~W[0]~, but what is that?
#+end_hint
#+end_exercise


#+begin_src python
plt.clf()
plt.plot(J)
plt.savefig("figures/sojourn.pdf")
#+end_src


#+begin_exercise
Change the simulation length to 1000 jobs.
Do one run for $\mu=3.5$ and another for $2.8$.
Compute the KPIs, make a plot, and include that in your assignment. Comment on what you see.
#+end_exercise

** Server KPI: idle time

This code computes the total time the server is idle, and then computes the fraction of time the server is idle.

#+begin_src python
rho = S.sum() / D[-1]
idle = (D[-1] - S.sum()) / D[-1]
print(idle)
#+end_src

#+begin_exercise
Explain the code above. Some specific points:
\begin{enumerate}
\item Why is ~S.sum()~ the total busy time of the server?
\item Why do we divide by ~D[-1]~ in the computation of $\rho$?
\item Explain the computation of the ~idle~ variable.
\end{enumerate}
#+end_exercise

The next code computes the separate idle times.
#+begin_src python
idle_times = np.maximum(A[1:] - D[:-1], 0)
print(idle_times)
print(idle_times.sum())
print(D[-1] - S.sum())
#+end_src


#+begin_exercise
Run this code for a simulation with 10 or so jobs (some other small number).
Explain how this code works. Which line is a check on the computations?
#+end_exercise

** Server KPI: busy time

We also like to know how a long the server has to work uninterruptedly. Finding the busy times is quite a bit harder than the idle times. (A busy time starts when a job arrives at an empty system and it stops when the server becomes free again.)

#+begin_exercise
To help you understand the code, let's first do a numerical example.
Suppose jobs $1, 4, 8$ find an empty system upon /arrival/. The simulation contains 10 jobs. Why do jobs $3, 7, 10$ leave an empty system behind upon /departure/?
#+end_exercise

With this idea, we can compute the idle times in another way (as a check on earlier work), and then we extend the approach to the busy times.

#+begin_src python
import numpy as np

np.set_printoptions(suppress=True)
np.random.seed(3)

num = 10
labda = 3
X = np.random.exponential(scale=1 / labda, size=num)
X[0] = 0
A = X.cumsum()
mu = 1.2 * labda
S = np.random.exponential(scale=1 / mu, size=len(A))
S[0] = 0
D = np.zeros_like(A)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]


W = D - S - A # waiting times
idx = np.argwhere(np.isclose(W, 0))
idx = idx[1:]  # strip A[0]
idle_times = np.maximum(A[idx] - D[idx - 1], 0)
print(idle_times.sum())
#+end_src

#+RESULTS:
: 1.3992207179039684

#+begin_exercise
What is stored in ~idx~?  Why do we strip ~A[0]~? Why do we subtract ~D[idx-1]~ and not ~D[idx]~? (Print out the variables to understand what they mean, e.g., ~print(idx)~.)
#+end_exercise


Now put the next piece of code behind the previous code so that we can compute the busy times.

#+begin_src python
busy_times = D[idx - 1][1:] - A[idx][:-1]
last_busy = D[-1] - A[idx[-1]]
print(busy_times.sum() + last_busy, S.sum())
#+end_src

#+begin_exercise
Explain these lines. About the last line, explain why this acts as a check.
#+end_exercise


** Virtual waiting time

Plotting the virtual waiting time is subtle.  (The code below is short, hence may seem to be easy to find, but for me it wasn't. To get it right took me two to three hours, and I also discovered other bugs I made elsewhere. Coding is hard!)

#+begin_exercise
Make a plot of the virtual waiting time by hand (you don't have a make a large plot, just show that you understand what the virtual waiting process looks like). Find out which points are the most important ones to characterize the virtual waiting times, and explain why this is so.
#+begin_hint
The crucial points are $(A_k, W_k)$, $(A_k, W_k+S_k)$, and $(D_k,0)$ when $W_{k-1} = 0$. Then connect these points with straight lines.
#+end_hint
#+end_exercise

Here is the code to plot the virtual waiting time.
#+begin_src python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(3)

num = 40
labda = 1
mu = 1.1 * labda
T = 10  # this acts as the threshold
X = np.random.exponential(scale=1 / labda, size=num)
X[0] = 0
A = np.zeros_like(X)
A = X.cumsum()
S = np.ones(len(A)) / mu
S[0] = 0
D = np.zeros_like(A)

W = np.zeros_like(A)
for k in range(1, len(X)):
    W[k] = max(W[k - 1] + S[k - 1] - X[k], 0)
    D[k] = A[k] + W[k] + S[k]

idx = np.where(W <= 0)[0] # this
empty = D[idx[1:] - 1]

E = np.zeros((2 * len(A) + len(empty), 2))  # epochs
E[: len(A), 0] = A
E[: len(A), 1] = W
E[len(A) : 2 * len(A), 0] = A
E[len(A) : 2 * len(A), 1] = W + S
E[2 * len(A) : 2 * len(A) + len(empty), 0] = empty
E[2 * len(A) : 2 * len(A) + len(empty), 1] = 0
E = E[np.lexsort((E[:, 1], E[:, 0]))]

plt.plot(E[:, 0], E[:, 1])
plt.savefig("figures/virtual-waiting-time.pdf")
#+end_src

The ~this~ line is perhaps somewhat strange.
By printing the result, we can find out that ~np.where(W <= 0)~ returns a tuple of which the first element is an array of the indices where ~W~ is zero.
To get that first element we add the extra ~[0]~.

#+begin_exercise
1. Explain how we store the relevant epochs in ~E~.
2. Why do we use ~idx[1:]~ (What is ~W[0]~)?
3. Why do we subtract 1 from ~idx[1:]~?
4. Why do we use ~np.lexsort~? (Check the documentation to see how lexical sorting works. It is important to know what lexical sorting is.)
#+end_exercise


* Computing the number of jobs in the system

We have the waiting times, but not the number of jobs in the system (queue). Here we show how to plot $L$, i.e., the number of jobs in the system as seen by a job /upon/ arrival.

A simple, but /inefficient/, algorithm to construct $L$ is the following.

#+begin_src python
import numpy as np

np.random.seed(3)

num = 10
labda = 1
mu = 1.1 * labda

X = np.random.exponential(scale=1 / labda, size=num)
X[0] = 0
A = np.zeros_like(X)
A = X.cumsum()
S = np.random.exponential(scale=1 / mu, size=len(A))
D = np.zeros_like(A)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

L = np.zeros_like(A)
for k in range(1, len(A)):
    idx = 0
    while D[idx] < A[k]:
        idx += 1
    L[k] = k - idx

print(L)
#+end_src


#+begin_exercise
1. Explain how this algorithm  works.
2. Why does this algorithm find the number of jobs as seen /just before the arrival/ of a job?
3. What line should be changed so that we count the number of jobs in the system /just after/ the arrival? What should it become?
4. Why is this algorithm (very) inefficient?
#+end_exercise



Making the algorithm much more efficient is not hard.
#+begin_src python
L = np.zeros_like(A)
idx = 0
for k in range(1, len(A)):
    while D[idx] < A[k]:
        idx += 1
    L[k] = k - idx
#+end_src

#+begin_exercise
Why is this much better?
#+begin_hint
The improved algorithm in $O(n)$, while the previous was $O(n^2)$.
#+end_hint
#+end_exercise


#+begin_exercise
Here is another algorithm that plots $\{L(t)\}$. Explain how it works, and its difference with the previous algorithm.

#+begin_src python :eval no-export
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(3)

num = 4
labda = 3
X = np.random.exponential(scale=1 / labda, size=num)
A = np.zeros(len(X) + 1)
A[1:] = X.cumsum()
mu = 0.3 * labda
S = np.random.exponential(scale=1 / mu, size=len(A))
S[0] = 0
D = np.zeros_like(A)

for k in range(1, len(A)):
    D[k] = max(D[k - 1], A[k]) + S[k]

L = np.zeros((len(A) + len(D), 2))
L[: len(A), 0] = A
L[1 : len(A), 1] = 1
L[len(D) :, 0] = D
L[len(D) + 1 :, 1] = -1
N = np.argsort(L[:, 0], axis=0)
L = L[N]
L[:, 1] = L[:, 1].cumsum()
print(L)

plt.clf()
plt.step(L[:, 0], L[:, 1], where='post', color='k')
plt.plot(A[1:], np.full_like(A[1:], -0.3), '^b', markeredgewidth=1)
plt.plot(D[1:], np.full_like(D[1:], -0.3), 'vr', markeredgewidth=1)
plt.savefig("figures/wait4.pdf")
#+end_src
#+end_exercise



* Multi-server queue

Let us now generalize the simulation to a $G/G/c$ queue, i.e., there are are $c\geq 1$ servers present to  serve jobs.
However, before doing that, we approximate the $G/G/c$ queueing process by a $G/G/1$ queue in which the server works $c$ times as fast. Like this we can reuse the code above to approximate a $G/G/c$ queue.


** A single fast server

While you are /still in queue/ of a multi-server queue, the rate at which jobs are served is the same whether there are $c$ servers or just one server working at a rate of $c$. As a first simple case, we model the multi-server queueing system as if there is one fast server working at rate $c$.

#+begin_exercise
Explain that we can implement a fast server by changing the service times as follows:
#+begin_src python
c = 3
S = np.random.exponential(scale=1 / (c * mu), size=len(A))
#+end_src
#+end_exercise

Here is the same idea, but the implementation is slightly different. This is useful because we will see that we can generalize this to a multi-server queue in which the servers have different speeds.
#+begin_src python
c = 3
W = np.zeros_like(S)
for k in range(len(S)):
    W[k] = max(W[k - 1] + S[k - 1]/c - X[k], 0)
#+end_src


** A real multi-server queue

Here is the code to implement the $G/G/2$ queue; see the queueing book to understand  the algorithm. (I include the print statements so that you can see step by step how it works.)

There is one very convenient feature of numpy, which is used in the ~this~ line of the $G/G/2$ queue code below. The feature is called /broadcasting/ and is best explained with an example.

#+begin_exercise
Run this code and explain what it does.
#+begin_src python #:results output :exports both
import numpy as np

a = np.array([2, 3, 4])
a = a - 1 # or a -= 1
print(a)
#+end_src
#+end_exercise

This is the code for the $G/G/2$ queue.
#+begin_src python
import numpy as np

num = 5

X = np.ones(num + 1)
S = 5 * np.ones(num)

c = np.array([1.0, 1.0])
W = np.zeros_like(S)  # store the waiting time as seen by the kth job
w = np.zeros_like(c)  # waiting times at each of the servers
for k in range(1, len(S)):
    s = w.argmin()  # server with smallest waiting time
    W[k] = w[s]
    print(f"{k=}", w, W[k])
    w[s] += S[k]
    print(w)
    print(w - X[k + 1])
    w = np.maximum(w - X[k + 1], 0)  # this
    print(w)

print(W)
#+end_src

#+begin_exercise
Run the code, and explan the results.
#+end_exercise

As as a test we could set the vector of server capacities like ~c=[1]~ so that we reduce our multi-server queue to a single-server queue, and compare the output with our earlier simulators. BTW: such `dumb' corner cases are necessary to test code.
In fact, it has happened many times that I tested code of which I was convinced it was correct, but I still managed to make bugs.
A bit of paranoia is a good state of mind when it comes to coding.

Now that we have tested the implementation (in part), here is the code for a queue served by three servers, but they can work at different speeds.

#+begin_src python
import numpy as np

np.random.seed(3)

labda = 3
mu = 1.1
N = 1000

X = np.random.exponential(scale=1 / labda, size=N + 1)
S = np.random.exponential(scale=1 / mu, size=N)

c = np.array([1.0, 1.0, 1.0])
W = np.zeros_like(S)
w = np.zeros_like(c)
for k in range(len(S)):
    s = w.argmin()
    W[k] = w[s]
    w[s] += S[k] / c[s]
    w = np.maximum(w - X[k + 1], 0)

print(W.mean(), W.std())
#+end_src

#+begin_exercise
Where in the code do we handle the fact that servers can work at different speeds?
#+end_exercise


#+begin_exercise
Compare $\E W$ for two cases. The first is a  $G/G/1$ queue with a single fast server working at rate $c=3$. The second is a model with three servers each working at rate $1$. Include your numerical results and discuss the differences.
#+begin_hint
Take ~c=[3.]~ for the first case, and ~c=[1., 1., 1.]~ for the second. With this, the change in code is minimal.
#+end_hint
#+end_exercise


#+begin_exercise
Change the code for the multi-server such that the individual servers have different speeds but the total service capacity remains the same. What is the impact on $\E W$ and $\V W$ as compared to the case with equal servers? Include your numerical results.
#+begin_hint
For instance, set ~c=np.array([2, 0.5, 0.5])~.
#+end_hint
#+end_exercise

#+begin_exercise
Once you researched the previous exercise, provide some consultancy advice.
Is it better to have one fast server and several slow ones, or is it better to have 3 equal servers?
What gives the smallest average queueing time and variance?
If the variance is larger when the service rates in the multi-server queue  are different  rates, explain the effects based on the intuition you can obtain from Sakasegawa's formula.
#+end_exercise




* TODO Restore my emacs settings                                   :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 100)
#+end_src
