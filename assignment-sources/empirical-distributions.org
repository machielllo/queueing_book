#+title: Convergence of distribution functions, empirical distributions.
#+author: Nicky D. van Foreest

#+STARTUP: indent
#+STARTUP: showall
#+PROPERTY: header-args:shell :exports both
#+PROPERTY: header-args:emacs-lisp :eval no-export
# +PROPERTY: header-args:python :eval no-export
#+PROPERTY: header-args:python :session  :exports both   :dir "./figures/" :results output

#+OPTIONS: toc:nil author:nil date:nil title:t

#+LATEX_CLASS: subfiles
#+LATEX_CLASS_OPTIONS: [assignments]

#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat "\\chapter{%t}\n"))
#+end_src

* TODO

- compute Wk, make plot of ecdf for k=1..20, k=21..40, etc. Include the graphs  in one plot and see how the ecdfs converge.


* Introduction

With simulation we can get estimates for the cdf of rvs. For instance,
for a queueing  system we might be interested in  the fraction of jobs
that see, upon  arrival, more than some number of  jobs in the system.
However, transient effects  can have a large impact.  For instance, if
the queue starts at a very high level, we first have to wait until the
system is drained. Only after the  system has been empty for the first
time, we can start estimating the cdf of the queue length process in a
sensible way.

In this assignment we will study of how the pdf changes over time.
First we concentrate on theoretical aspects, then we'll study how things work with simulation.
We'll see that the problem of estimating the pdf is not a simple undertaking.
You should remember that data analysis is tricky, and to do things right, you really have to understand what you are doing.
(For students that like maths, we will be concerned with weak convergence.
If you're not interesting in fundamental probability, you can just forget this term.)

In passing we will develop some very nice, and useful, coding concepts with python.

* Sums of uniform rvs

By the Central Limit Theorem (CLT) we know that the  sum of iid rvs $\{X_{k}\}$ (with a sufficient number of finite moments) converge to the normal distribution. In more detail, if $\mu := \E X$, $\sigma := \sqrt{\V X}$, with $X_{k}\sim X$, and $S_n = \sum_{k=1}^n X_k$,
\begin{align*}
\frac{S_n-n \mu}{\sqrt{n} \sigma} \to \Norm{0, 1}, \quad \textrm{as } n\to \infty.
\end{align*}

Let us obtain some intuitive understanding of this limit by making plots of the pdfs of $S_n$ for various values of $n$.  For this, we need to find a good way to add iid rvs.

** Computing the pdf of a sum of two rvs

Here is the code to compute the pdf of the sum of two uniform rvs.

#+begin_src python
from collections import defaultdict
import matplotlib.pyplot as plt

U = {1: 0.5, -1: 0.5}

S2 = defaultdict(float)
for i, pi in U.items():
    for j, pj in U.items():
        S2[i + j] += pi * pj


print(S2)
support = sorted(S2.keys())
pdf = [S2[i] for i in support]

plt.plot(support, pdf)
plt.savefig("figures/S2.pdf")
#+end_src

#+begin_exercise
Read the python documentation for =defaultdict=, so that you understand how that works and why that is very useful here.
1. Explain how the code uses two nested for loops to compute the pdf of ~S~.
2. What is the ~support~ of ~S~?
3. Change the values of ~U~ to something of your liking, compute ~S~ and include the figure in your report (so that I can check that you really ran the code.)
#+begin_hint
Recall that when $X$ and $Y$ are independent rvs with densities $f_X$ and $f_Y$, then
\begin{align*}
f_{X\pm Y}(n) &= \sum_i \sum_j f_X(i) f_Y(j)\1{i\pm j = n}.
\end{align*}
#+end_hint

#+end_exercise


** Sum of three rvs

By repeating the code above,  we can compute the sum of three rvs.

#+begin_src python
from collections import defaultdict
import matplotlib.pyplot as plt

U = {1: 0.5, -1: 0.5}

S2 = defaultdict(float)
for i, pi in U.items():
    for j, pj in U.items():
        S2[i + j] += pi * pj


print(S2)

S3 = defaultdict(float)
for i, pi in S2.items():
    for j, pj in U.items():
        S3[i + j] += pi * pj

print(S3)
#+end_src

If we need to compute $S_n$, $n\geq 4, it is obvious that we need more and more code like this. This is ugly business, so we need smarter concepts.

* More general, and much better, code to work with rvs

To avoid all such  tedious repetition, we can use the concept of a /class/. Using classes and object orientation is the main reason why I like python a lot over R. Classes scale and give much cleaner code, i.e., code that easier to understand, get correct, document, and maintain.

** A first, simple class

Here is a first step.

#+begin_src python
from collections import defaultdict


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def sum(self, Y):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[i + j] += pi * pj # this
        return R


U = RV({1: 0.5, -1: 0.5})
S2 = U.sum(U)

print(S2)
#+end_src

#+begin_exercise
1. Read the python documentation to see how ~__init__~ works for a python class. (Just read, you don't have to copy it.)
2. Explain why we call =super().__init__(float)=, i.e., what does this do? Hint: Search the web on what is =super()= in python.
#+end_exercise

** A more general class

With a bit of computer knowledge, we can spot a general pattern if we think about how to subtract two rvs, rather than add. In particular, we would write ~R[i-j]~ instead of ~R[i+j]~ in the ~this~ line above. If we would want to compute the product of two rvs, we should change the operator ~+~ by ~*~. So, now that we realize we deal with general /operators/, let's use that to improve our code.

#+begin_src python
from collections import defaultdict
import operator


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def apply_operator(self, Y, op):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[op(i, j)] += pi * pj
        return R

    def __add__(self, X):
        return self.apply_operator(X, operator.add)

    def __sub__(self, X):
        return self.apply_operator(X, operator.sub)


U = RV({1: 0.5, -1: 0.5})
S = U + U + U
print(S)
#+end_src

Observe the following points:
1. In the method =apply_operator= we pass on a reference to an operator ~op~.
2. The methods ~__sub__~ and =__add__= specify the type of operator.

#+begin_exercise
What does the following code do?
#+begin_src python
    def __mul__(self, X):
        return self.apply_operator(X, operator.mul)
#+end_src
Add this to the class ~RV~ above (mind that it's properly indented) . Then run the code below, and explain the result.

#+begin_src python
T = U * U
print(T)
#+end_src
#+end_exercise

I hope you see the power of classes. With very little extra work, we get very general, extensible, and clear, code. Adding division is really easy too, the operator is called =truediv=.

** Applying functions to rvs

Recall that in the computation of the waiting times for a queueing system, we need the $[x]^{+} = \max\{x, 0\}$ function.
That is, $W_k=\max\{W_{k-1} + S_{k-1} - X_{k}, 0\}$. Including such functions is easy by extending the ~RV~ just a bit more.

#+begin_src python
from collections import defaultdict
import operator


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def apply_operator(self, Y, op):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[op(i, j)] += pi * pj
        return R

    def apply_function(self, f):
        R = RV()
        for i, pi in self.items():
            R[f(i)] += pi
        return R

    def __add__(self, X):
        return self.apply_operator(X, operator.add)

    def __sub__(self, X):
        return self.apply_operator(X, operator.sub)

    def pos(self):
        return self.apply_function(lambda x: max(x, 0))


U = RV({1: 0.5, -1: 0.5})
print(U.pos())
#+end_src

#+begin_exercise
Run this, and explain  the results for ~U.pos()~.
#+begin_hint
It is well-known that
\begin{align*}
f_{h(X)}(n) &= \sum_{i} f_X(i)\1{h(i)=n}.
\end{align*}
#+end_hint

#+end_exercise


** Plotting the pmf

If we want to plot  the pmf, we need two extra methods pmf in the class ~RV~.

#+begin_src python
    def supp(self):
        return sorted(self.keys())

    def pmf(self):
        return [self[k] for k in self.supp()]
#+end_src

#+begin_exercise
Add this code to ~RV~. (Mind again, what you add, should be properly indented.) Then run the code for this example:
#+begin_src python
U = RV({1: 0.5, -1: 0.5})
S = U + U
S += S
S += S

plt.plot(S.supp(), S.pmf())
plt.savefig("figures/St.pdf")
#+end_src
Is the final ~S~ the sum of 2, 4, or 8  uniform rvs? Why?
#+end_exercise

** Comparison with the normal distribution

Let us compare the pmf of a sum of a bunch of discrete uniform rvs to the pdf of a normal rv. This is harder than you might think.


#+begin_exercise
Run this code.  Add this code to the code for ~RV~ (and remove other code that you don't use anymore).

#+begin_src python
U = RV({1: 0.5, -1: 0.5})
S = U + U

for i in range(3):
    S += S

supp = S.supp()
delta = supp[1] - supp[0]
plt.plot(supp, S.pmf(), label="S")
plt.plot(supp, delta * norm.pdf(supp, scale=4))
plt.legend()
plt.savefig("figures/Snorm.pdf")
#+end_src

Explain why we have to set the scale to 4, and why we have to multiply the pdf of the normal by ~delta~ to get a pmf.
(Some motivational remarks: of course I forgot the scale and the ~delta~ at first.
To repair, I included the scale.
Then the result was still not OK, but I recalled that an extra factor, the ~delta~, is also necessary.)

#+end_exercise



* Convergence of the pdf of the waiting times

Suppose that $X_k$ is uniformly distributed on the set $\{1,2,4\}$ and $S_k$ uniform on the set $\{1,2,3\}$, so that $\rho<1$.
Starting with $W_{0}=5$, we like to construct the \emph{distribution} of the waiting times with the rule $W_{k}=[W_{k-1}+S_{k-1}-X_k]^+$.
Observe that this rule contains three steps.
1. The sum of two rvs: $Z_k = W_{k-1} + S_{k-1}$,
2. The difference of two rvs:  $Z_k' = Z_k - X_k$
3. Apply the function $[\cdot]^{+}$: $[Z_k']^+$.
If you have studied the code above, you should immediately conclude that we already have all code to compute and plot the pmf of $W_k$ for increasing values of $k$.

Here  is the code.

#+begin_src python
W = RV({30: 1})
X = RV({1: 1 / 3, 2: 1 / 3, 4: 1 / 3})
S = RV({1: 1 / 3, 2: 1 / 3, 3: 1 / 3})

for n in range(1, 101):
    W += S - X
    W = W.pos()
    if n % 10 == 0:
        plt.plot(W.supp(), W.pmf(), label="k={}".format(n))


plt.axis([0, 50, 0, 0.3])
plt.legend()
plt.savefig("figures/w-dists.pdf")
#+end_src

#+begin_exercise
Run the code, include the graph, and explain what you see.
#+end_exercise


#+begin_exercise
Let's do a longer run. Replace the  code above by this code. Then run it, and explain what you see.
#+begin_src python

for n in range(1, 101):
    W += S - X
    W = W.pos()


for n in range(100, 301):
    W += S - X
    W = W.pos()
    if n % 50 == 0:
        plt.plot(W.supp(), W.pmf(), label="k={}".format(n))


plt.axis([0, 50, 0, 0.3])
plt.legend()
plt.savefig("figures/w-dists2.pdf")
#+end_src
#+end_exercise



* Using simulation to estimate the transient cdf of the waiting times

With the tools above we can compute how the cdf $F_{k}$, say, of $W_k$ behaves over time, i.e., as a function of $k$. It is apparent that $F_k$ converges to some limiting cdf $F$, say. (Don't forget, we are not proving anything with our numerical work; we just make certain claims plausible.)

Suppose that, instead of using the ~RV~ class to compute $F_k$, we would use simulation to /estimate/ $F_k$, how would we fare? This is what we'll work on in the remainder of this document.

The ECDF is easier to get then the EPDF. The latter requires the selection of bins.

We want to know the fraction of periods the queue length is longer than some value $q$, say. For this we will make the empirical distribution of the queue lengths.

** The empirical distribution function

Compute the pmf of simulated (or measured) data is always a bit awkward because we have to specify the bins in which we want to `throw the data'. A way to avoid this, is by using the /empirical distribution function (ecdf)/ rather than the pmf (estimated with bins).  Before we explain how  to compute the ecdf, here is the definition. Given a set of measurements $x_{1}, \ldots, x_n$, the ecdf is defined as
\begin{align*}
F(x) = \frac{1}{n}\sum_{i=1}^n \1{x_i \leq x}.
\end{align*}


Notwithstanding that this is a clean mathematical definition, we should stay clear from using it to /compute/ the ecdf (the numerical performance is absolutely terrible).
To make a ecdf, we follow three steps:
1. Sort the measurements.
2. Count how often each value occurs.
3. Normalize, so as to get a real cdf, i.e., a non-decreasing function with $\lim_{x\to -\infty} F(x) = 0$, and $\lim_{x\to\infty} F(x) = 1$.

#+begin_exercise
Suppose we measured the following set of interarrival times: $2, 5, 2, 1, 9, 5, 5, 5$. Use the three steps above to compute the ecdf /by hand/, so that you really understand how it works.
#+end_exercise

Here is some code that you should use to compare your answers.

#+begin_src python
import numpy as np
import matplotlib.pyplot as plt

def ecdf(x):
    support, values = np.unique(x, return_counts=True)
    return support, values.cumsum() / values.sum()


x = [2, 5, 2, 1, 9, 5, 5, 5]
x, F = ecdf(x)
print(x, F)

plt.plot(x, F, drawstyle="steps-post")
plt.savefig("figures/ecdf.pdf")
#+end_src

#+RESULTS:
: [1 2 5 9] [0.125 0.375 0.875 1.   ]


#+begin_exercise
Read the numpy documentation on ~np.unique~ to understand what this function does. Why don't we have to sort ~x~ before calling ~np.unique~?
#+end_exercise

#+begin_exercise
Make the plot of ~F~, and discuss that the boundaries at the left and right are not fully ok. (We can repair this, but that is a bit work, which we don't do here.)
#+end_exercise


** Application to simulation of waiting times

Here is the code.

#+begin_src python
import itertools
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(3)


def ecdf(x):
    support, values = np.unique(x, return_counts=True)
    return support, values.cumsum() / values.sum()


num = 1 + 1000
X = np.random.choice([1, 2, 4], size=num)
S = np.random.choice([1, 2, 3], size=num)
W = np.zeros(num)
W[0] = 30
step = num // 10

markers = itertools.cycle(['o', 's', 'v', '.'])

for n in range(1, num):
    W[n] = max(W[n - 1] + S[n - 1] - X[n], 0)
    if n % step == 0:
        x, F = ecdf(W[n - step : n])
        plt.plot(x, F, label=f"{n=}", marker=next(markers), markersize=3)

x, F = ecdf(W)
plt.plot(x, F, label=f"W")
plt.legend()
plt.savefig("figures/w_ecdf.pdf")
#+end_src

#+begin_exercise
1. Explain how this simulaton works.
2. We include the 1 in ~num~to ensure that also the last part of the run is plotted. Remove the $+1$ to see what happens; that will reveal the problem.
3. You can skip the cycling through the markers, but it's a fun trick to know.
#+end_exercise

#+begin_exercise
Make plots with ~num = 1000 +1~, ~num = 10000 + 1~ and ~num = 50000 + 1~, and include these plots. Discuss what you see. If you conclude that data analysis with simulation is difficult, then you learned something really important.
#+end_exercise



* Restore my emacs settings   :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 100)
#+end_src

#+RESULTS:
