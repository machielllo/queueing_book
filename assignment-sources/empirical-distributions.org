#+title: Convergence of distribution functions, empirical distributions.
#+author: Nicky D. van Foreest

#+STARTUP: indent
#+STARTUP: showall
#+PROPERTY: header-args:shell :exports both
#+PROPERTY: header-args:emacs-lisp :eval no-export
#+PROPERTY: header-args:python :eval no-export
# +PROPERTY: header-args:python :session  :exports both   :dir "./figures/" :results output

#+OPTIONS: toc:nil author:nil date:nil title:t

#+LATEX_CLASS: subfiles
#+LATEX_CLASS_OPTIONS: [assignments]

#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat "\\chapter{%t}\n"))
#+end_src

* TODO

- compute Wk, make plot of ecdf for k=1..20, k=21..40, etc. Include the graphs  in one plot and see how the ecdfs converge.


* Introduction

With simulation we can get estimates for the cdf of random variables.
For instance, for a queueing system we might be interested in the fraction of jobs that see, upon arrival, more than some number of jobs in the system.
However, transient effects can have a large impact.
For instance, if the queue starts at a very high level, we first have to wait until the system is drained.
Only after the system has been empty for the first time, we can start estimating the cdf of the queue length process in a sensible way.

In this assignment we will study of how the pdf changes over time.
First we concentrate on theoretical aspects, then we'll study how things work with simulation.
We'll see that the problem of estimating the pdf is not a simple undertaking.
You should remember that data analysis is tricky, and to do things right, you really have to understand what you are doing.
(For students that like maths, we will be concerned with weak convergence.
If you're not interesting in fundamental probability, you can just forget this term.)

In passing we will develop some very nice, and useful, coding concepts with python.

* Sums of uniform rvs

By the Central Limit Theorem (CLT) we know that the  sum of iid rvs $\{X_{k}\}$ (with a sufficient number of finite moments) converge to the normal distribution. In more detail, if $\mu := \E X$, $\sigma := \sqrt{\V X}$, with $X_{k}\sim X$, and $S_n = \sum_{k=1}^n X_k$,
\begin{align*}
\frac{S_n-n \mu}{\sqrt{n} \sigma} \to \Norm{0, 1}, \quad \textrm{as } n\to \infty.
\end{align*}

Let us obtain some intuitive understanding of this limit by making plots of the pdfs of $S_n$ for various values of $n$.  For this, we need to find a good way to add iid rvs.

** Computing the pdf of a sum of two rvs

Here is the code to compute the pdf of the sum of two uniform rvs.

#+begin_src python
from collections import defaultdict
import matplotlib.pyplot as plt

U = {1: 0.5, -1: 0.5}

S2 = defaultdict(float)
for i, pi in U.items():
    for j, pj in U.items():
        S2[i + j] += pi * pj


print(S2)
support = sorted(S2.keys())
pdf = [S2[i] for i in support]

plt.plot(support, pdf)
plt.savefig("figures/S2.pdf")
#+end_src

#+begin_exercise
Read the python documentation for =defaultdict=, so that you understand how that works and why that is very useful here.
1. Explain how the code uses two nested for loops to compute the pdf of ~S~.
2. What is the ~support~ of ~S~?
3. Change the values of ~U~ to something of your liking, compute ~S~ and include the figure in your report (so that I can check that you really ran the code.)
#+begin_hint
Recall that when $X$ and $Y$ are independent rvs with densities $f_X$ and $f_Y$, then
\begin{align*}
f_{X\pm Y}(n) &= \sum_i \sum_j f_X(i) f_Y(j)\1{i\pm j = n}.
\end{align*}
#+end_hint

#+end_exercise


** Sum of three rvs

By repeating the code above,  we can compute the sum of three rvs.

#+begin_src python
from collections import defaultdict
import matplotlib.pyplot as plt

U = {1: 0.5, -1: 0.5}

S2 = defaultdict(float)
for i, pi in U.items():
    for j, pj in U.items():
        S2[i + j] += pi * pj


print(S2)

S3 = defaultdict(float)
for i, pi in S2.items():
    for j, pj in U.items():
        S3[i + j] += pi * pj

print(S3)
#+end_src

If we need to compute $S_n$, $n\geq 4, it is obvious that we need more and more code like this. This is ugly business, so we need smarter concepts.

* More general, and much better, code to work with rvs

To avoid all such  tedious repetition, we can use the concept of a /class/. Using classes and object orientation is the main reason why I like python a lot over R. Classes scale and give much cleaner code, i.e., code that easier to understand, get correct, document, and maintain.

** A first, simple class

Here is a first step.

#+begin_src python
from collections import defaultdict


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def sum(self, Y):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[i + j] += pi * pj # this
        return R


U = RV({1: 0.5, -1: 0.5})
S2 = U.sum(U)

print(S2)
#+end_src

#+begin_exercise
1. Read the python documentation to see how ~__init__~ works for a python class. (Just read, you don't have to copy it.)
2. Explain why we call =super().__init__(float)=, i.e., what does this do? Hint: Search the web on what is =super()= in python.
#+end_exercise

** A more general class

With a bit of computer knowledge, we can spot a general pattern if we think about how to subtract two rvs, rather than add. In particular, we would write ~R[i-j]~ instead of ~R[i+j]~ in the ~this~ line above. If we would want to compute the product of two rvs, we should change the operator ~+~ by ~*~. So, now that we realize we deal with general /operators/, let's use that to improve our code.

#+begin_src python
from collections import defaultdict
import operator


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def apply_operator(self, Y, op):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[op(i, j)] += pi * pj
        return R

    def __add__(self, X):
        return self.apply_operator(X, operator.add)

    def __sub__(self, X):
        return self.apply_operator(X, operator.sub)


U = RV({1: 0.5, -1: 0.5})
S = U + U + U
print(S)
#+end_src

Observe the following points:
1. In the method =apply_operator= we pass on a reference to an operator ~op~.
2. The methods ~__sub__~ and =__add__= specify the type of operator.

#+begin_exercise
What does the following code do?
#+begin_src python
    def __mul__(self, X):
        return self.apply_operator(X, operator.mul)
#+end_src
Add this to the class ~RV~ above (mind that it's properly indented) . Then run the code below, and explain the result.

#+begin_src python
T = U * U
print(T)
#+end_src
#+end_exercise

I hope you see the power of classes. With very little extra work, we get very general, extensible, and clear, code. Adding division is really easy too, the operator is called =truediv=.

** Applying functions to rvs

Recall that in the computation of the waiting times for a queueing system, we need the $[x]^{+} = \max\{x, 0\}$ function.
That is, $W_k=\max\{W_{k-1} + S_{k-1} - X_{k}, 0\}$. Including such functions is easy by extending the ~RV~ just a bit more.

#+begin_src python
from collections import defaultdict
import operator


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for i, pi in p.items():
                self[i] = pi

    def apply_operator(self, Y, op):
        R = RV()
        for i, pi in self.items():
            for j, pj in Y.items():
                R[op(i, j)] += pi * pj
        return R

    def apply_function(self, f):
        R = RV()
        for i, pi in self.items():
            R[f(i)] += pi
        return R

    def __add__(self, X):
        return self.apply_operator(X, operator.add)

    def __sub__(self, X):
        return self.apply_operator(X, operator.sub)

    def pos(self):
        return self.apply_function(lambda x: max(x, 0))


U = RV({1: 0.5, -1: 0.5})
print(U.pos())
#+end_src

#+begin_exercise
Run this, and explain  the results for ~U.pos()~.
#+begin_hint
It is well-known that
\begin{align*}
f_{h(X)}(n) &= \sum_{i} f_X(i)\1{h(i)=n}.
\end{align*}
#+end_hint

#+end_exercise


** Plotting the pmf

If we want to plot  the pmf, we need two extra methods pmf in the class ~RV~.

#+begin_src python
    def supp(self):
        return sorted(self.keys())

    def pmf(self):
        return [self[k] for k in self.supp()]
#+end_src

#+begin_exercise
Add this code to ~RV~. (Mind again, what you add, should be properly indented.) Then run the code for this example:
#+begin_src python
U = RV({1: 0.5, -1: 0.5})
S = U + U
S += S
S += S

plt.plot(S.supp(), S.pmf())
plt.savefig("figures/St.pdf")
#+end_src
Is the final ~S~ the sum of 2, 4, or 8  uniform rvs? Why?
#+end_exercise

** Comparison with the normal distribution

Let us compare the pmf of a sum of a bunch of discrete uniform rvs to the pdf of a normal rv. This is harder than you might think.


#+begin_exercise
Run this code.  Add this code to the code for ~RV~ (and remove other code that you don't use anymore).

#+begin_src python
U = RV({1: 0.5, -1: 0.5})
S = U + U

for i in range(3):
    S += S

supp = S.supp()
delta = supp[1] - supp[0]
plt.plot(supp, S.pmf(), label="S")
plt.plot(supp, delta * norm.pdf(supp, scale=4))
plt.legend()
plt.savefig("figures/Snorm.pdf")
#+end_src

Explain why we have to set the scale to 4, and why we have to multiply the pdf of the normal by ~delta~ to get a pmf.
(Some motivational remarks: of course I forgot the scale and the ~delta~ at first.
To repair, I included the scale.
Then the result was still not OK, but I recalled that an extra factor, the ~delta~, is also necessary.)

#+end_exercise




* Waiting times

Suppose that $X_k$ is uniformly distributed on the set $\{1,2,4\}$ and $S_k$ uniform on the set $\{1,2,3\}$, so that $\rho<1$.
Starting with $W_{0}=5$, we like to construct the \emph{distribution} of the waiting times with the rule $W_{k}=[W_{k-1}+S_{k-1}-X_k]^+$.
Observe that this rule contains three steps.
1. The sum of two rvs: $Z_k = W_{k-1} + S_{k-1}$,
2. The difference of two rvs:  $Z_k' = Z_k - X_k$
3. Apply the function $[\cdot]^{+}$: [Z_k']^+$.
If you have studied the code above, you should immediately conclude that we already have all code to compute and plot the pmf of $W_k$ for increasing values of $k$.

Here is code to compute the pmf of the waiting times.

#+begin_src python
W = RV({30: 1})
X = RV({1: 1 / 3, 2: 1 / 3, 4: 1 / 3})
S = RV({1: 1 / 3, 2: 1 / 3, 3: 1 / 3})

for n in range(1, 101):
    W += S - X
    W = W.pos()
    if n % 10 == 0:
        plt.plot(W.supp(), W.pmf(), label="k={}".format(n))


plt.axis([0, 50, 0, 0.3])
plt.legend()
plt.savefig("figures/w-dists.pdf")
#+end_src

#+begin_exercise
Run the code, include the graph, and explain what you see.
#+end_exercise


#+begin_exercise
Let's do a longer run. Replace the  code above by this code. Then run it, and explain what you see.
#+begin_src python

for n in range(1, 101):
    W += S - X
    W = W.pos()


for n in range(100, 301):
    W += S - X
    W = W.pos()
    if n % 50 == 0:
        plt.plot(W.supp(), W.pmf(), label="k={}".format(n))


plt.axis([0, 50, 0, 0.3])
plt.legend()
plt.savefig("figures/w-dists2.pdf")
#+end_src
#+end_exercise



* Restore my emacs settings   :noexport:

#+begin_src emacs-lisp :eval no-export
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 100)
#+end_src

#+RESULTS:


* Empirical distribution, how to make and plot

The ECDF is easier to get then the EPDF. The latter requires the selection of bins.

We want to know the fraction of periods the queue length is longer than some value $q$, say. For this we will make the empirical distribution of the queue lengths.

** Plotting a PDF/histogram

#+begin_src python :results value file
import matplotlib.pyplot as plt

x = [2, 5, 2, 1, 9, 5, 5, 5]

plt.clf()
plt.hist(x, bins=3, facecolor='red', edgecolor='black', linewidth=1)
plt.savefig('emp0.pdf')
#+end_src

#+begin_src python :results value file :exports results
'emp0.pdf'
#+end_src

#+RESULTS:
[[file:figures/emp0.pdf]]


#+begin_exercise
What happens if  you set ~bins=10~ and then run the simulation? Include a graph to show this.
#+end_exercise

** First naive idea

Given a set of measurements $x_{1}, \ldots, x_n$, the empirical CDF is defined as
\begin{align*}
F(x) = \frac{1}{n}\sum_{i=1}^n \1{x_i \leq x}
\end{align*}
This is a clean mathematical definition, but as if often the case with mathematical definitions, you should stay clear from using it to /compute/ the CDF: the numerical performance is absolutely terrible.


#+begin_src python
x = [2, 5, 2, 1, 8, 5, 5]

def F(y):
    tot = 0
    for xi in x:
        tot += xi <= y # this

    return  tot/len(x)

print(F(5.5))
#+end_src

#+RESULTS:
: 0.875

#+begin_exercise
Explain first how the ~this~ line works.
#+begin_hint
Run this code to see what happens
#+begin_src python
print(3 < 4)
jj = 0
jj += 3 < 4
print(jj)
#+end_src
#+end_hint
#+end_exercise

#+begin_exercise
Explain in your own words why this way to compute the empirical CDF is a not so smart (i.e., pretty dumb) idea.
#+begin_hint
How many comparisons are required for each value of $F(y)$?
#+end_exercise

** A better idea

#+begin_src python
print(sorted(x))
#+end_src

#+RESULTS:
: [1, 2, 5, 8]


#+begin_src python
plt.clf()
plt.plot(sorted(x))
plt.savefig("emp00.pdf")
#+end_src

#+begin_src python :results value file :exports results
"emp00.pdf"
#+end_src

#+RESULTS:
[[file:figures/emp00.pdf]]



** Yet better idea

#+begin_src python
def cdf_better(x):
    x = sorted(x)
    n = len(x)
    y = range(1, n + 1)
    y = [z / n for z in y]  # normalize
    return x, y


x = [2, 5, 2, 1, 8, 5, 5]
x, F = cdf_better(x)
print(F)
#+end_src

#+RESULTS:
| 0.14285714285714285 | 0.2857142857142857 | 0.42857142857142855 | 0.5714285714285714 | 0.7142857142857143 | 0.8571428571428571 | 1.0 |

#+begin_exercise
Why do we divide by $n$ to normalize?
#+end_exercise

You should know that ~for~ loops in R and python are quite slow. We use this in the list comprehension in the line in which we ~#normalize~.
For larger amounts of data it is better to use =numpy=. This we do below.


** Plot the cdf

#+begin_src python
x = [2, 5, 2, 1, 8, 5, 5, 5]
x, F = cdf_better(x)
#+end_src


#+begin_src python
plt.clf()
plt.step(x, F)
plt.savefig("emp2.pdf")
#+end_src

#+RESULTS:

#+begin_src python :results value file :exports results
"emp2.pdf"
#+end_src

#+RESULTS:
[[file:figures/emp2.pdf]]


#+begin_src python
plt.clf()
plt.plot(x, F, drawstyle="steps-post")
plt.savefig("emp3.pdf")
#+end_src

#+RESULTS:

#+begin_src python :results value file :exports results
"emp3.pdf"
#+end_src

#+RESULTS:
[[file:figures/emp3.pdf]]

#+begin_exercise
Explain the differences between these different graphs. Which is the correct one? (Read the code well to see what happens.)
#+end_exercise


** Faster with numpy

#+begin_src python
import numpy as np

def cdf(x):
    y = np.arange(1, len(x) + 1) / len(x)
    x = np.sort(x)
    return x, y

x = [2, 5, 2, 1, 8, 5, 5]
x, F = cdf(x)
print(F)
#+end_src

#+RESULTS:
| 0.14285714 | 0.28571429 | 0.42857143 | 0.57142857 | 0.71428571 | 0.85714286 | 1 |

** Remove duplicate values

Finally, we can make the computation of the cdf significantly faster with using the following numpy functions.

#+begin_src python
unique, count = np.unique(np.sort(x), return_counts=True)
print(unique, count)
#+end_src

#+RESULTS:
| array | ((1 2 5 8)) | array | ((1 2 3 1)) |

#+begin_exercise
Explain what ~np.unique~ does and how this improves the speed of the computation of the ECDF (empirical CDF). What happens if you forget to sort the input ~x~?
#+end_exercise


#+begin_src python
print(count.cumsum()/7)
#+end_src

#+RESULTS:
| 0.14285714 | 0.42857143 | 0.85714286 | 1 |

#+begin_exercise
What is ~cumsum~?
#+end_exercise

#+begin_src python
def cdf_fastest(x):
    # remove multiple occurences of the same value
    unique, count = np.unique(x, return_counts=True)
    x = unique
    y = count.cumsum() / count.sum()
    return x, y

x = [2, 5, 2, 1, 8, 5, 5]
x, F = cdf_fastest(x)
print(F)
#+end_src

#+RESULTS:
| 0.14285714 | 0.42857143 | 0.85714286 | 1 |

#+begin_exercise
Find the CDF for arrival times [2,5,7,8,9,10] and plot it.
#+end_exercise


#+begin_exercise
Explain what  the Kolmogorov-Smirnov test has to do with the ECDF; search on the web for this. Keep your discussion short, but to the point.
#+end_exercise
