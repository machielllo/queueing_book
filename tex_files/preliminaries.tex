% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: clean: { extensions: [ aux, blg, idx, ilg, ind, log, out, pytxcode, rel, toc ] }
% !arara: clean: { files: [ ans.tex, hint.tex] }

\documentclass[queueing_book]{subfiles}
\externaldocument{queueing_book}


\loadgeometry{normal}

\begin{document}

In the first section of this chapter we provide some high level motivation why to study queueing systems and an overview of the book. In the second we recall some notation and results of analysis and probability that we use throughout the book.

\section{Motivation and overview}
\label{sec:motivation-overview}

Queueing systems abound, and the analysis and control of queueing systems are major topics in the control, performance evaluation and optimization of production and service systems.


\newthought{At my local supermarket}, for instance, any customer gets his/her groceries for free when there is a queue of 4 or more customers and there is an unoccupied cashier desk.
The manager that controls the occupation of the cashier positions is keen on keeping the queue short.
Now this is easy enough: just hire many cashiers. However, the cost of personnel may then outweigh the yearly average cost of paying the customer penalties.
Thus, the manager's problem is to control the service capacity such the penalties and the personnel cost are about equally large.

Fast food restaurants also deal with many interesting queueing situations.
Consider, for instance, the preparation of hamburgers.
Typically, hamburgers are made-to-stock, in other words, they are prepared before the actual demand has arrived.
Thus, hamburgers in stock can be interpreted as customers in queue waiting for service, where the service time is the time between the arrival of two customers that buy hamburgers.
The hamburgers have a typical lifetime, and they have to be scrapped if they remain on the shelf longer than a specified amount of time.
Of course, it is easy to achieve zero scrap cost, simply by keeping no stock at all.
However, to prevent lost-sales, it is important to maintain a certain amount of hamburgers in stock.
In this case, the manager has to balance the scrap cost against the cost of lost sales.

Service systems, such as hospitals, call centers, courts, and so on, have a certain capacity available to serve customers.
The performance of such systems is, in part, measured by the total number of jobs processed per year and the fraction of jobs processed within a certain time frame between receiving and closing the job.
Here the problem is to organize the capacity such that the sojourn time, i.e., the typical time a job spends in the system, does not exceed some threshold.

\newthought{Clearly, in all} these examples, the performance of the queueing system has to be monitored and controlled.
Typically the following performance measures are relevant.
\begin{enumerate}
\item The fraction of time $p(n)$ that the system contains $n$ customers.
 In particular, $1-p(0)$, i.e., the fraction of time the system contains jobs, is important, as this is a measure of the time-average occupancy of the servers, hence related to personnel cost.
\item The fraction of customers $\pi(n)$ that `see upon arrival' the system with $n$ customers.
  This measure relates to customer perception and lost sales, i.e., fractions of arriving customers that do not enter the system.
\item The average, variance, and/or distribution of the waiting time.
\item The average, variance, and/or distribution of the number of customers in the system.\
\end{enumerate}
In this book, we will be primarily concerned with making models of queueing systems such that we can compute or estimate these performance measures.

\newthought{In~\cref{cha:single-stat-queu} we} start with constructing queueing systems in discrete time and continuous time.
This serves three goals.
First, construction is concrete, so that by specifying the rules to characterize the behavior of the system, you (the reader) develop essential modeling skills.
Second, these rules can often be easily implemented in computer code and used to simulate actual queueing system.
Simulation is in general the best way to analyze practical queueing systems, as realistic systems seldom yield to mathematics.
Third, we will use sample-path arguments to develop the theoretical results of~\cref{cha:analytical-models} and onwards, and these sample paths are precisely what simulators produce.

\newthought{Notwithstanding the power} of simulation, it is often hard to obtain structural understanding of the behavior of queueing systems.
Instead, mathematical models, whether exact or approximate, are useful to help reason about and improve queueing systems.
In~\cref{cha:approximate-models} we use approximations and general results of probability theory to understand how production and service situations are affected by the system parameters such as service speed, batching rules, and outages.
As such, the first two chapters illustrate and motivate the study of practical queueing systems,

\newthought{Once it is} clear what queueing theory is about, the stage is set for a more mathematical treatment of queueing systems.
In \cref{cha:fundamental-tools} we develop some necessary key results.
For this, we use sample paths of queueing process as a guiding principle, and assume that sample paths capture the `normal' stochastic behavior of the system (with probability one.)
Since we can also construct sample paths with simulation, sample paths form a direct bridge between the practical aspects of queueing theory of~\cref{cha:single-stat-queu} and the mathematical analysis of the rest of the book.


\newthought{In~\cref{cha:analytical-models} we} develop exact models for single-station queueing systems and in \cref{cha:queu-contr-open} we extend our study to simple examples of queueing control and queueing networks.
Here we combine the material of the earlier chapters with other mathematical tools such as difference and differential equations, and non-negative matrices.
As such, the last chapter in particular provides a stepping stone to Markov chains, Markov decision theory, optimization and dynamic programming.

\newthought{In our discussions} we mostly focus on obtaining an intuitive understanding of the analytical tools.
For proofs and/or more extensive results we refer to the bibliography at the end of the book.


\newthought{While the main} text contains some examples and derivations,  most of the examples are delegated to exercises.
Also, some of these exercises are consistency checks between results derived for different queueing models, and thereby provide important relations between various parts of the text.
We note in passing that, while such checks are trivial in principle, the algebra can be quite difficult at times.
The exercises are not meant to be really easy; they should require (some) work.
Hints and solutions to all problems.
are available at the end of the book.


\section{Indispensable knowledge}
\label{sec:indep-knowl}



\opt{solutionfiles,check}{
\loadgeometry{tufte}
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}

\newthought{There are some} fundamental concepts you are supposed to have seen earlier in your career.
We recall them here, and use them over and over in the rest of the book, mostly without reference.

\newthought{We introduce the} following notation:
 \begin{align*}
 \1{A} &=
 \begin{cases}
 1, & \text{if $A$ is true}, \\
 0, & \text{if $A$ is false}.
 \end{cases},
& [x]^+ &= x \1{x\geq 0}, \\
 f(x-) &= \lim_{y\uparrow x} f(y), &
 f(x+) &= \lim_{y\downarrow x} f(y).
\end{align*}
The (set) function $\1{}$ is known as the \recall{indicator function}.

We write $f(h)=o(h)$ for a function $f$ to say that $f$ is such that $f(h)/h \to 0$ as $h\to 0$.
If we write $f(h) = o(h)$ it is implicit that $|h| \ll 1$.
We call this \recall{small $o$ notation}.


You should know that:
\begin{subequations}
 \begin{align}
 (a+b)^n &= \sum_{i=0}^n {n \choose i} a^{n-i} b^i, \label{eq:71}\\
e^x &= \lim_{n\to\infty} (1+x/n)^n, \label{eq:65}\\
 e^x &= \sum_{k=0}^{\infty} \frac{x^k}{k!}, \label{eq:76}\\
 \sum_{n=0}^N \alpha^n &= \frac{1-\alpha^{N+1}}{1-\alpha}. \label{eq:61}
\end{align}
\end{subequations}

\newthought{For a non-negative}, integer-valued random variable $X$ with \recall{probability mass function} $f(k)= \P{X = k}$,
\recall{distribution function} $F(k)=\P{X\leq k}$ and \recall{survivor function} $G(k) = \P{X>k}$ we have
\begin{subequations}\label{eq:105}
\begin{align}
X&=\sum_{n=0}^\infty X\1{X=n} = \sum_{n=0}^\infty n \1{X=n}, \label{eq:77} \\
\E{\1{X\leq x}} &= \P{X\leq x}, \label{eq:74}\\
\E X &= \E{\sum_{n=0}^\infty n \1{X=n}} = \sum_{n=0}^\infty n \E{\1{X=n}} = \sum_{n=0}^\infty n f(n), \label{eq:p-78} \\
  \E{g(X)} &= \sum_{n=0}^\infty g(n) f(n), \label{eq:66} \\
  \V X &= \E{X^2} - (\E X)^2.\label{eq:68}\\
\end{align}
\end{subequations}
Eq.~\cref{eq:66} is known as the \recall{law of the unconscious statistician}.


Let $X$ be a continuous non-negative random variable with distribution function $F$.
We write $ \E{X} = \int_0^\infty x F(\d x)$ for the expectation of~$X$.
Here $F(\d x)$ acts as a (sort of) shorthand for $f(x) \d x$.\sidenote{For details, see \citet{capinski03:_probab_probl}.}
Recall that $\E{g(X)} = \int_0^\infty g(x) F(\d x)$.

You should be able to use indicator functions and integration by parts to show that $\E{X^2} = 2\int_0^\infty y G(y) \d y$, where $G(x) = 1- F(x)$, provided the second moment exists.

For general random variables $X$ and $Y$:
\begin{align}
  \E{X+Y} &= \E X + \E Y, \label{eq:95}\\
\intertext{and, if $X$ and $Y$ are independent so that $\E{XY} = \E X \E Y$,}
  \V{X+Y} &= \V X + \V Y. \label{eq:94}
\end{align}

\newthought{The \recall{moment-generating function}} $M_X(s)$ of a random variable~$X$ is defined for~$s\in \mathbb{R}$ sufficiently small as:
\begin{subequations}
\begin{align}
 M_X(s) &= \E{e^{s X}}. \label{eq:75}\\
\intertext{$M_X(s)$  uniquely characterizes the distribution of $X$. From this definition it follows that:}
 \E{X} &= M_{X}'(0) = \left.\frac{\d M_{X}(s)}{d s}\right|_{s=0},\label{eq:69}\\
\E{X^2} &= M_{X}''(0), \label{eq:64}\\
\intertext{and, if $X$ and $Y$ are independent,}
M_{X+Y}(s) &= M_X(s)\cdot M_Y(s). \label{eq:73}
\end{align}
\end{subequations}


\newthought{About \recall{conditional probability}} you should know that
\begin{subequations}\label{eq:70}
\begin{align}
\P{A\given B} & = \frac{\P{AB}}{\P{B}}, \quad\text{if}\, \P{B}>0, \\
\intertext{and, if $A=\bigcup_{i=1}^n B_i$ with $\P{B_i>0}$ for all $i$,}
 \P{A} &= \sum_{i=1}^n \P{A B_i} = \sum_{i=1}^n \P{A\given B_i} \P{B_i}.
\end{align}
\end{subequations}

\begin{exercise}\label{ex:12}
Show that $|h|^{\alpha} = o(h)$ for all $\alpha > 1$. Conclude in particular that $a h^{2} = o(h)$ for any constant $a$.
\begin{solution}
Take $f(x) = h^{\alpha}$. Then,
\begin{align*}
\lim_{h\to 0} \frac{f(h)}{h} =
\lim_{h\to 0} \frac{h^{\alpha}}{h} =
\lim_{h\to 0} h^{\alpha-1}.
\end{align*}
\end{solution}
\end{exercise}




\begin{exercise}\label{ex:l-104}
 Let $c$ be a constant (in $\R$) and the functions $f$ and $g$ both of $o(h)$. Then show that (1) $f(h) \to 0$ when $h\to 0$, (2) $c\cdot f = o(h)$, (3) $f+g=o(h)$, and (4) $f\cdot g=o(h)$.
\begin{solution}
 In fact (1) is trivial: $|f(h)| \leq |f(h)/h|$ when $|h| < 1$.
 But it is given that the RHS goes to zero.
 For (2) and (3):
\begin{align*}
\lim_{h\to 0} \frac{c f(h)}{h} &= c \lim_{h\to 0} \frac{f(h)}{h} = 0, \; \text{as } f = o(h), \\
\lim_{h\to 0} \frac{f(h) + g(h)} h &= \lim_{h\to 0} \frac{f(h)} h + \lim_{h\to 0} \frac{g(h)} h = 0.
\end{align*}
For (4), use the Algebraic Limit Theorem and multiply with $h/h$,
\begin{align*}
\lim_{h\to 0} \frac{f(h)g(h)}{h} &= \lim_{h\to 0} h \frac{f(h)}{h} \frac{g(h)}{h}
= \lim_{h\to 0} h \lim_{h\to 0} \frac{f(h)}{h} \lim_{h\to 0} \frac{g(h)}{h} = 0
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:87}
 Why is $e^{x} = 1 +x + o(x)$?
\begin{solution}
 When $|x|\ll 1$, the terms with $n\geq 2$ in~\cref{eq:76} are $x^n = o(x)$. Then applying $x^n + x^m = o(x)$ to the Taylor series gives the result.
\end{solution}
\end{exercise}

\begin{exercise}
 Why is~\cref{eq:77} true?
\begin{solution}
To see~\cref{eq:77}, note first that $X\1{X=n} = n\1{X=n}$ because $X=n$ when $\1{X=n} =1$, and second that $\sum_{n=0}^\infty \1{X=n} =1$, since $X$ takes one of the values in $\N$, and events $\{X=n\}$ and $\{X=m\}$ are non-overlapping when $n\neq m$.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:l-105}
Show that $G(k) = \sum_{m=0}^\infty \1{m>k} f(m)$ for discrete $X$.
\begin{solution}
 This is just rewriting the definition:
\begin{equation*}
G(k) = \P{X>k} = \sum_{m=k+1}^\infty \P{X=m} = \sum_{m=k+1}^\infty f(m)=\sum_{m=0}^\infty \1{m>k}f(m).
\end{equation*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:6}
 For a nonnegative discrete random variable $X$, use indicator functions to prove that $\E X = \sum_{k=0}^\infty G(k)$.
\begin{hint}
Write
$\sum_{k=0}^\infty G(k) = \sum_{k=0}^\infty \sum_{m=k+1}^\infty \P{X=m}$, reverse the summations. Then realize that $\sum_{k=0}^\infty \1{k<m} = m$.
\end{hint}
\begin{solution}
Observe first that $\sum_{k=0}^\infty \1{m>k} = m$, since $\1{m>k}=1$ if $k<m$ and $\1{m>k} = 0$ if $k\geq m$. With this,
\begin{align*}
\sum_{k=0}^\infty G(k)
&= \sum_{k=0}^\infty \P{X>k}
= \sum_{k=0}^\infty \sum_{m=k+1}^\infty \P{X=m} \\
& = \sum_{k=0}^\infty \sum_{m=0}^\infty \1{m>k} \P{X=m}
= \sum_{m=0}^\infty \sum_{k=0}^\infty \1{m>k} \P{X=m} \\
&= \sum_{m=0}^\infty m\P{X=m} = \E X.
\end{align*}
There are some technical details with respect to the interchange of the summations.
Since the summands are positive, this is allowed.
For further detail, see \citet{capinski03:_probab_probl}.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:66}
For discrete $X$,  use indicator functions to prove that
$\sum_{i=0}^\infty i G(i) = \E{X^2}/2 - \E{X}/2$.
\begin{hint}
$\sum_{i=0}^\infty i G(i) = \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^\infty i \1{n\geq i+1}$,
and reverse the summations.
\end{hint}
\begin{solution}
\begin{align*}
\sum_{i=0}^\infty i G(i)
&= \sum_{i=0}^\infty i \sum_{n=i+1}^\infty \P{X=n} = \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^\infty i \1{n\geq i+1} \\
&= \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^{n-1}i = \sum_{n=0}^\infty \P{X=n} \frac{(n-1)n}{2} \\
&= \sum_{n=0}^\infty \frac{n^2}{2} \P{X=n} - \frac{\E X}{2}
= \frac{\E{X^2}}{2} - \frac{\E X}{2}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:l-107}
For  general non-negative $X$,  use indicator functions to prove that
$ \E X = \int_0^\infty x F(\d x) = \int_0^\infty G(y) \d y,$
where $G(x) = 1 - F(x)$.
\begin{hint}
$\E X = \int_0^\infty x F(\d x) = \int_0^\infty \int_0^\infty \1{y\leq x} \d y F(\d x)$.
\end{hint}
\begin{solution}
 \begin{align*}
 \E{X} &= \int_0^\infty x F(\d x) = \int_0^\infty \int_0^x \d y F(\d x) \\
 & = \int_0^\infty \int_0^\infty \1{y\leq x} \d y F(\d x) = \int_0^\infty \int_0^\infty \1{y\leq x} F(\d x) \d y\\
 & = \int_0^\infty \int_y^\infty F(\d x) \d y = \int_0^\infty G(y) \d y.
 \end{align*}
\end{solution}
\end{exercise}


\begin{exercise}
  Use indicator functions to prove that for a general non-negative random variable~$X$, $ \E{X^2} = 2 \int_0^\infty y G(y) \d y$.
\begin{hint}
$\int_0^\infty y G(y) \d y = \int_0^\infty y \int_0^\infty \1{y\leq x}f(x)\, \d x \d y$.
\end{hint}
\begin{solution}
 \begin{align*}
\int_0^\infty y G(y) \d y
&= \int_0^\infty y \int_y^\infty f(x)\, \d x \d y = \int_0^\infty y \int_0^\infty \1{y\leq x}f(x)\, \d x \d y\\
&= \int_0^\infty f(x) \int_0^\infty y \1{y \leq x}\, \d y \d x
= \int_0^\infty f(x) \int_0^x y\, \d y \d x\\
&= \int_0^\infty f(x) \frac{x^2}2 \d x =\frac{\E{X^2}}2.
 \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:l-108}
 Show that $\E{X^2}/2 = \int_0^\infty y G(y) \d y$ for a continuous non-negative random variable $X$ with survivor function $G$.
 \begin{solution}
 \begin{hint}
 Use integration by parts.
 \end{hint}
 \begin{equation*}
 \int_0^\infty y G(y) \d y
= \frac{y^2}2 G(y) \bigg|_0^\infty - \int_0^\infty \frac{y^2}2 g(y)\d y = \int_0^\infty \frac{y^2}2 f(y)\d y = \frac{\E{X^2}}2,
 \end{equation*}
 since $g(y) = G'(y) = - F'(y) = - f(y)$. Note that we used $\frac{y^2}2 G(y) \bigg|_0^\infty = 0 - 0 = 0$, which follows from our assumption that $\E{X^2}$ exists, implying that $\lim_{y \to \infty} y^2G(y) = 0$.
\end{solution}
\end{exercise}



\begin{exercise}
 What is the value of $M_X(0)$?
\begin{solution}
 $M_X(0) = \E{e^{0 X}} = \E{e^0} = \E{1} = 1$.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:l-109}
 We have one gift to give to one out of three children. As we cannot
 divide the gift into parts, we decide to let `fate decide'. That
 is, we choose a random number in the set $\{1, 2, 3\}$. The first
 child that guesses this number wins the gift. Show that the
 probability of winning the gift is the same for each child.
\begin{hint}
 For the second child, condition on the event that the first does not choose the right number.
 Use the definition of conditional probability:
 $\P{A\given B} = \P{AB}/\P{B}$ provided $\P{B}>0$.
\end{hint}
\begin{solution}
 The probability that the first child to guess  wins is
 $1/3$. What is the probability for child number two? Well, for
 him/her to win, it is necessary that child one does not win and
 that child two guesses the right number of the remaining
 numbers. Assume, without loss of generality that child 1 chooses
 $3$ and that this is not the right number. Then
 \begin{equation*}
 \begin{split}
&\P{\text{Child 2 wins}} \\
&= \P{\text{Child 2 guesses the right number and child 1 does not win}} \\
&= \P{\text{Child 2 guesses the right number} \given\, \text{child 1 does not win}}
\cdot \P{\text{Child 1 does not win}} \\
&= \P{\text{Child 2 makes the right guess in the set $\{1,2\}$}}\cdot \frac 23 \\
&= \frac 1 2\cdot \frac 23 = \frac 1 3.
 \end{split}
 \end{equation*}
 Similar conditional reasoning gives that child 3 wins with probability $1/3$.
\end{solution}
\end{exercise}

\input{trailer}
