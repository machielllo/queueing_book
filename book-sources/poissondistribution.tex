% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: clean: { extensions: [ aux, blg, idx, ilg, ind, log, out, pytxcode, rel, toc ] }
% !arara: clean: { files: [ ans.tex, hint.tex] }

\input{header}

\section{Poisson Distribution}\label{sec:poisson-distribution}

In this section we first introduce the Poisson process to model the arrival process of jobs\sidenote{A job can be anything that requires service; it can be a customer, an item to make, a car to repair, and so on.}
that need to receive service at a station.\sidenote{A station is a general name for a shop, a machine, a mechanic, and so on.}

In the exercises we derive numerous properties of the Poisson process; the rest of the book we will use these results time and again.



\newthought{Consider a stream of customers} that enter a shop during a time interval of duration $t=1$ hour.\sidenote{The unit of time $t$ depends on the context.
  For example, when we model a shop, it is reasonable to take $t$ in the order of one hour, but when we think about a certain type of operations at a hospital, the $t$ might be weeks.}
If we chop up the hour into small periods of 1 seconds, it is reasonable to assume that the probability of an arrival in a specific period is small and the probability of two or more arrivals is very small.
Moreover, it is reasonable to assume that the probability that a customer enters in a certain second is the same for each second.

More formally, to model such an arrival process, we make a few assumptions.
We think about the interval as composed of $n$ many small subintervals, all with equal length $t/n$.
We take $n$  so large that we can neglect the probability that two or more customers arrive in one small interval.
Moreover, we assume that the arrival of a customer in a certain subinterval is independent of the (non)-arrival of customers in any of the other subintervals, and that the probability of an arrival is given by a (very) small probability $p$.

In precise terms, we model the arrivals of customers as a set of \recall{identically and independently distributed (iid)}  random variables $\{B_i\}$  such that $B_i=1$ denotes an arrival in interval~$i$, and $B_i=0$ no arrival.  The arrival probability is $\P{B_i =1}=p$.

\recall{Bernoulli distributed}

It is well-known that the total number of arrivals $N_n(t)$ that occur in the $n$ intervals is \recall{binomially distributed}, i.e.,
\begin{equation*}
 \P{N_n(t) = k} = {n \choose k} p^k (1-p)^{n-k}.
\end{equation*}
It is easy to show\sidenote{\cref{ex:p1}} that  the expected number of arrivals during $[0,t]$ is
$  \E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$.



\newthought{Let us take} the limit $n\to\infty$ and $p\to0$, but such that the expected number of arrivals $n p$ during $[0,t]$ remains constant.
Specifically, we take the limit such that $p n =\lambda t$, where the constant $\lambda$ has the interpretation of the \recall{arrival rate} of  customers.
In this case,\sidenote{\cref{ex:31}}
\begin{equation}\label{eq:bin}
  \lim_{\stackrel{n\to\infty, p\to 0}{np=\lambda t}}{n \choose k} p^k (1-p)^{n-k} = e^{-\lambda t}\frac{(\lambda t)^k}{k!}.
\end{equation}


We say that the random variable $N(t)$
associated with this distribution is \recall{Poisson distributed}, i.e.,
\begin{equation*}
 \P{N(t) = k} =
e^{-\lambda t} \frac{(\lambda t)^k}{k!},
\end{equation*}
and we write $N(t)\sim \Pois(\lambda t)$.\sidenote{\cref{ex:p2}}
With $N(t)$, we define $N(s, t] := N(t)-N(s)$ as the number of customers arriving in the time period $(s, t]$.\sidenote{Note that $[0,t]$ is closed at both ends, but $(s,t]$ is open at the left.}



\newthought{The family} of random variables $N_\lambda=\{N(t), t\geq 0\}$ is a \recall{Poisson process} with rate $\lambda$.\sidenote{ A random process is a much more complicated mathematical object than a random variable: a process is a (possibly uncountable) set of random variables indexed by time, not just \emph{one} random variable.}
We remark that  $N_\lambda$ has \recall{stationary and independent increments}.
Stationarity means that the distributions of the number of arrivals are the same for all intervals of equal length, that is,
$N(s,t]$ has the same distribution as $N(u, v]$ if $t-s = v-u$.
Independence means, roughly speaking, that knowing that $N(s,t]= n$, does not help to make any predictions about the value of $N(u, v]$ if the intervals $(s,t]$ and $(u, v]$ do not overlap.\sidenote{We refer to the literature on (mathematical) probability theory for further background.}

We next address a number of useful properties of the Poisson process.
If $\Delta t\ll 1$ then for all $t$,\sidenote{~\cref{ex:35}--\cref{ex:p-35}}
\begin{subequations}\label{eq:32}
\begin{align}
\P{N(t+\Delta t) = n \given N(t) = n} &= 1-\lambda \Delta t + o(\Delta t),  \label{eq:4}\\
\P{N(t+\Delta t) = n+1 \given N(t) = n} &= \lambda \Delta t + o(\Delta t), \label{eq:8} \\
\P{N(t+\Delta t) \geq n+2 \given N(t) = n} &= o(\Delta t).\label{eq:10}
\end{align}

The next equation says that if you know that an arrival occurred during $[0,t]$, the arrival is distributed uniformly on the interval $[0,t]$.
If $s\in [0,t]$,\sidenote{\cref{ex:p-36} }
\begin{equation}\label{eq:22}
  \P{N(s) =1\given N(t)=1} = \frac s t.
\end{equation}
Note that this is independent of $\lambda$.
Finally,\sidenote{~\cref{ex:2}--\cref{ex:pe-53}}
\begin{align}
\E{N(t)} &= \lambda t,  \label{eq:11}\\
\E{(N(t))^2} &= \lambda^2 t^2 + \lambda t,\label{eq:19}\\
\V{N(t)} &= \lambda t.\label{eq:21}
\end{align}
\end{subequations}

\newthought{The relative variability} of service times is a very important concept in queueing theory.
For instance, suppose the standard deviation of customer inter-arrival times is $1$ minute.
When the mean inter-arrival time is 1 hour, we are inclined to call the process regular, while if the mean is 1 minute, we would call it irregular.
To differentiate between such cases, define the \recall{square coefficient of variation (SCV)} of a random variable~$X$ as
\begin{equation}\label{eq:62}
 C^2:= \frac{\V X}{(\E X)^2}.
\end{equation}



\newthought{Merging and splitting} of arrival processes often occurs in practice.
Consider, for instance, the arrival processes $N_\lambda$ of men and $N_\mu$ of women at a shop, see the figure at the right.
\begin{marginfigure}
\begin{tikzpicture}[xscale=0.3]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);
\end{tikzpicture}
\end{marginfigure}
Each cross represents an arrival; in the upper line it corresponds to a man, in the middle line to a woman, and in the lower line to an arrival of a general customer at the shop.
Thus, the shop `sees' the merged process of these two arrival processes.
In fact, this merged process $N_{\lambda+\mu}$ is also a Poisson process\sidenote{\cref{ex:l-103} } with rate $\lambda+\mu$.

We can also \emph{split}, or \emph{thin}, a stream into several sub-streams.
Model the stream of people passing by a shop as a Poisson process $N_\lambda$.
In the figure at the right, we mark these arrivals as crosses at the upper line.
\begin{marginfigure}
\begin{tikzpicture}[xscale=0.4]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(6,2);
\node[left] at (0,2) {$N_\lambda$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(6,0);
\node[left] at (0,0) {$N_{\lambda p}$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06)
node[below] {$B_1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3)
node[below] {$B_2$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3)
node[below, fill=white] {$B_3$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06)
node[below] {$B_4$};

% \draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06)
% node[below] {$B_5=1$};


% \draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3)
% node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
 \end{marginfigure}
With probability $p$ a person decides, independent of anything else, to enter the shop; the crosses at the lower line are the customers that enter the shop.
The Bernoulli random variable $B_1=1$ so that the first passerby enters the shop; the second passerby does not enter as $B_2=0$, and so on.



The concepts of \recall{merging} and \recall{thinning} are useful to analyze queueing networks, see~\cref{sec:jackson-networks}.
Suppose the departure stream of a machine splits into two sub-streams, e.g., a fraction $p$ of the jobs moves on to another machine and the rest ($1-p$) of the jobs leaves the system.
Then we can model the arrival stream at the second machine as a thinned stream (with probability $p$) of the departures of the first machine.
Merging occurs where the output streams of various stations arrive at another station.

\newthought{With moment-generating functions} we can simplify the derivations above; the last few exercises of this section show how to apply this.\sidenote{In general, it is hard to obtain closed-form expressions for the moment-generating function, but when it works, it is an easy and slick technique.}


\begin{exercise}\label{ex:p1}
Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$.
\begin{hint}
Use that $\E{X+Y} = \E X + \E Y$.
\end{hint}
\begin{solution}
 \begin{equation*}
 \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
 \end{equation*}
\end{solution}
\end{exercise}

\begin{exercise} \label{ex:31}
 Show~\cref{eq:bin}
\begin{hint}
 First find $p$, $n$, $\lambda$ and $t$ such that the rate at which an event occurs in both processes are the same.
 Then consider the binomial distribution and use the standard limit $(1-x/n)^n \to e^{-x}$ as $n\to \infty$.
\end{hint}
\begin{solution}
 \begin{align*}
 {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k}
&= \frac{n!}{k!(n-k)!} \left(\frac{\lambda t}{n}\frac{n}{n-\lambda t}\right)^k \left(1-\frac{\lambda t}n\right)^{n} \\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n!}{n^k(n-k)!}\left(1-\frac{\lambda t}n\right)^{n}\\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n}{n}\frac{n-1}{n}\cdots\frac{n-k+1}{n} \left(1-\frac{\lambda t}n\right)^{n}.
\end{align*}
Observe now that, as $\lambda t$ is finite, $n/(n-\lambda t)\to 1$ as
$n\to \infty$. Also for any finite $k$, $(n-k)/n\to1$. Finally, use~\cref{eq:65} to see that
$\left(1-\frac{\lambda t}n\right)^{n} \to e^{-\lambda t}$.

\end{solution}
\end{exercise}


\begin{exercise}\label{ex:p2}
What is the difference between $N_n(t)$ and $N(t)$?
\begin{solution}
 $N_n(t)$ is a binomially distributed random variable with parameters $n$ and $p$.
 The maximum value of $N_n(t)$ is $n$.
 The random variable $N(t)$ models the number of arrivals that can occur during $[0,t]$.
 As such it is not necessarily bounded by $n$.
 Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable.
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:35}
 Show \cref{eq:4}.
\begin{hint}
Use the definition of the conditional probability and small $o$ notation.

\end{hint}
\begin{solution}
Write $N(s, t]$ for the number of arrivals in the interval $(s,t]$. First we make a few simple observations: $N(t+\Delta t]= N(t) + N(t, t+\Delta t]$, hence
\begin{align*}
 \1{N(t+\Delta t)=n, N(t)=n}
&= \1{N(t) + N(t, t+\Delta t] = n, N(t)=n} = \1{N(t, t+\Delta t] = 0, N(t)=n}.
\end{align*}
Thus,
 \begin{align*}
 \P{N(t+\Delta t) = n \given N(t) = n}
&= \frac{\P{N(t+\Delta t) = n, N(t) = n}}{\P{N(t)=n}} \\
&= \frac{\P{N(t, t+\Delta t] = 0, N(t) = n}}{\P{N(t)=n}} \\
&= \frac{\P{N(t, t+\Delta t] = 0} \P{N(t) = n}}{\P{N(t)=n}} \quad \text{(independence)}\\
%& \quad\text{(by independence of } N(t, t+h] \text{ and } N(t))\\
&= \P{N(t, t+\Delta t] = 0}
= \P{N(0, \Delta t] = 0} \quad\text{(stationarity)} \\
&= e^{-\lambda \Delta t} (\lambda \Delta t)^0/0!
= e^{-\lambda \Delta t} = 1-\lambda \Delta t + o(\Delta t).
 \end{align*}
\end{solution}
\end{exercise}



\begin{exercise} \label{ex:p-355}
 Show \cref{eq:8}.
\begin{hint} Use~\cref{ex:35} and \cref{ex:12}.
\end{hint}
\begin{solution}
 \begin{align*}
 \P{N(t+\Delta t) = n +1 \given N(t) = n}
&= \frac{\P{N(t+\Delta t) = n +1 , N(t) =n}}{P{N(t) = n}}\\
&= \P{N(t, t+\Delta t] = 1} = e^{-\lambda \Delta t} \frac{(\lambda \Delta t)^1}{1!} \\
&= (1-\lambda \Delta t + o(\Delta t))\lambda \Delta t = \lambda \Delta t - \lambda^2 \Delta t^2 + o(\Delta t) \\
&= \lambda \Delta t + o(\Delta t).
 \end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{ex:p-35}
 Show \cref{eq:10}
\begin{hint}
See the hint for~\cref{ex:p-355}, and  use $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x - 1$.
\end{hint}
\begin{solution}
 \begin{align*}
 \P{N(t+\Delta t) \geq n+2 \given N(t) = n}
&= \P{N(t, t+\Delta t] \geq 2} \\
&= e^{-\lambda \Delta t} \sum_{i=2}^\infty \frac{(\lambda \Delta t)^i}{i!}
= e^{-\lambda \Delta t} \left(\sum_{i=0}^\infty \frac{(\lambda \Delta t)^i}{i!} - \lambda \Delta t - 1\right)\\
&= e^{-\lambda \Delta t}(e^{\lambda \Delta t} - 1 - \lambda \Delta t)
= 1 - e^{-\lambda \Delta t}(1 + \lambda \Delta t) \\
&= 1 - (1-\lambda \Delta t + o(\Delta t))(1+\lambda \Delta t)
= 1 - (1-\lambda^2 \Delta t^2 + o(\Delta t)) \\
&= \lambda^2 \Delta t^2 + o(\Delta t) = o(\Delta t),
 \end{align*}
where we expand
\begin{align*}
(1-\lambda \Delta t + o(\Delta t))(1+\lambda \Delta t)
&= 1 + \lambda \Delta t - \lambda \Delta t - \lambda^{2} \Delta^{2} t^{2} + o(\Delta t).
\end{align*}


We can also use the results of the previous parts to see that
\begin{align*}
 \P{N(t+\Delta t) \geq n+2 \given N(t) = n}
&= \P{N(t, t+\Delta t] \geq 2} = 1- \P{N(t, t+\Delta t]<2} \\
&= 1 - \P{N(t, t+\Delta t]= 0} - \P{N(t, t+\Delta t]=1} \\
&= 1 - (1-\lambda \Delta t + o(\Delta t) ) - (\lambda \Delta t + o(\Delta t)) \\
&= o(\Delta t).
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:2}
 Show \cref{eq:11}.
\begin{hint}
Use~\cref{eq:66}. Note that the term with $n=0$ does not contribute in the following summation
\begin{equation*}
\sum_{n=0}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty \frac{\lambda^n}{(n-1)!} = \lambda \sum_{n=0}^\infty \frac{\lambda^n}{n!} = \lambda e^{\lambda},
\end{equation*}
where we apply a change of notation in the second to last step.
\end{hint}
\begin{solution}
 When a random variable~$N$ is Poisson distributed with parameter
 $\lambda t$,
 \begin{align*}
 \E N
&= \sum_{n=0}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}
= \sum_{n=1}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}
= e^{-\lambda t} \lambda t \sum_{n=1}^\infty \frac{(\lambda t)^{n-1}}{(n-1)!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty \frac{(\lambda t)^{n}}{n!}
= e^{-\lambda t} \lambda t e^{\lambda t}
= \lambda t.
 \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:p-1}
 Show \cref{eq:19}.
\begin{solution}
 \begin{align*}
 \E{N^2}
&= \sum_{n=0}^\infty n^2 e^{-\lambda t}\frac{(\lambda t)^n}{n!}
= e^{-\lambda t} \sum_{n=1}^\infty n \frac{(\lambda t)^n}{(n-1)!}
= e^{-\lambda t} \sum_{n=0}^\infty (n+1) \frac{(\lambda t)^{n+1}}{n!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty n \frac{(\lambda t)^{n}}{n!} +e^{-\lambda t}\lambda t \sum_{n=0}^\infty\frac{(\lambda t)^{n}}{n!}
= (\lambda t)^2 + \lambda t.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:pe-53}
 Show \cref{eq:21}.
\begin{hint} Use~\cref{eq:68}, \cref{ex:p-1},  and~\cref{ex:2}.
\end{hint}
\begin{solution}
$\V N = \E{N^2} - (\E N)^2 = (\lambda t)^2 + \lambda t - (\lambda t)^2 = \lambda t$.
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:p-36}
 Show \cref{eq:22}.
\begin{hint}
 Observe that
 \begin{equation*}
%\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
\1{N(0,s]+N(s,t]=1}\1{N(0,s]=1} = \1{1+N(s,t]=1}\1{N(0,s]=1}=\1{N(s,t]=0}\1{N(0,s]=1}.
 \end{equation*}
Use independence and~\cref{eq:74}.
\end{hint}
\begin{solution}
From the hint,
\begin{align*}
 \P{N(0,s] =1\given N(0,t]=1}
&= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}}
= \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} = \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:l-102}
 Show that the SCV of $N(t)\sim P(\lambda t)$ is equal to $1/(\lambda t)$. What does this mean for $t$ large?
\begin{solution}
 \begin{equation*}
SCV = \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{\lambda t}.
 \end{equation*}
The relative variability of the Poisson process goes down as $t\to\infty$.
\end{solution}
\end{exercise}



\begin{exercise}\label{ex:l-103}
If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show with a conditioning argument that
the merged process $N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$.
\begin{hint}
 Use sets $\{N_\lambda(t) = i\}$ to decompose $\{N_\lambda(t) + N_\mu(t) = n\}$. With this observe that
 \begin{equation*}
 \1{N_\lambda(t) + N_\mu(t) = n} =
 \sum_{i=0}^n \1{N_\lambda(t)=i, N_\mu(t) = n-i}.
 \end{equation*}
Take expectations left and right, use~\cref{eq:74}, and independence of $N_\lambda$ and $N_\mu$. Near the end of the computation, use~\cref{eq:71}.
\end{hint}
\begin{solution}
\begin{align*}
\P{N_\lambda(t) + N_\mu(t) = n}
&= \sum_{i=0}^n \P{N_\mu(t) = n-i}\P{N_\lambda(t)=i} \\
&= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t}
= e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} \\
&= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i \quad\text{(binomial formula)} \\
&= \frac{((\mu+\lambda)t)^n}{n!}e^{-(\mu+\lambda)t}.
 \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:30a}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show that
 \begin{equation*}
 \P{N_\lambda(t) = 1 \given N_\lambda(t) + N_\mu(t) = 1} =
\frac{\lambda}{\lambda+\mu}.
 \end{equation*}
 Note that the RHS does not depend on $t$.
\begin{hint}
Use the standard formula for conditional probability and that $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$. Interpret the result.
\end{hint}
\begin{solution}
 With the above:
 \begin{align*}
& \P{N_\lambda(t) = 1 \given N_\lambda(t) + N_\mu(t) = 1}
= \frac{\P{N_\lambda(t) = 1, N_\lambda(t) + N_\mu(t) = 1}}{\P{N_\lambda(t) + N_\mu(t) = 1}} \\
&= \frac{\P{N_\lambda(t) = 1, N_\mu(t) = 0}}{\P{N_{\lambda+\mu}(t) = 1}}
= \frac{\P{N_\lambda(t) = 1}\P{N_\mu(t) = 0}}{\P{N_{\lambda+\mu}(t) = 1}} \\
&= \frac{\lambda t \exp(-\lambda t) \exp(-\mu t)}{((\lambda+\mu)t)\exp{(-(\lambda+\mu)t)}}
= \frac{\lambda t \exp{(-(\lambda + \mu)t)}}{((\lambda+\mu)t)\exp{(-(\lambda+\mu)t)}}
= \frac{\lambda}{\lambda+\mu}.
 \end{align*}

 Given that a customer arrived in $[0,t]$, the probability that it is of the first type is $\lambda/(\lambda+\mu)$.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:32}
 Show with conditioning that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
\begin{hint}
Suppose that $N_1$ is the thinned stream, and $N$ the original stream. Condition on the total number of arrivals $N(t)=n$ up to time
 $t$. Then, realize that the probability that a person is of type 1 is $p$. Hence, when you consider $n$ people in
 total, the number $N_1(t)$ of type 1 people is binomially distributed. Thus, given that $n$ people arrived, the probability of $k$ `successes' (i.e., arrivals of type 1), is
 \begin{equation*}
 \P{N_1(t)=k \given N(t) = n} = {n \choose k} p^k (1-p)^{n-k}.
 \end{equation*}
Use~\cref{eq:70} to decompose the $\{N_1=k\}$, and~\cref{eq:76} at the end.
\end{hint}
\begin{solution}
\begin{align*}
 \P{N_1(t) = k}
&= \sum_{n=k}^\infty \P{N_1(t) =k, N(t) = n}
= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}\P{N(t)=n} \\
&= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
&= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda t} \frac{(\lambda t)^n}{n!}, \quad\text{by the hint}\\
&= e^{-\lambda t}\sum_{n=k}^\infty \frac{p^k (1-p)^{n-k}}{k! (n-k)!} (\lambda t)^n
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=k}^\infty \frac{(\lambda t (1-p))^{n-k}}{(n-k)!}\\
&= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=0}^\infty \frac{(\lambda t (1-p))^{n}}{n!}
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} e^{\lambda t(1-p)} = e^{-\lambda t p} \frac{(\lambda t p)^k}{k!}.
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:53}
Show that $M_{N(t)}(s) = \exp{(\lambda t(e^s-1))}$.
\begin{hint}
Use~\cref{eq:66} with $f(x) = e^{sx}$.
\end{hint}
\begin{solution}
Since $N(t)$ is Poisson distributed with parameter $\lambda t$,
\begin{align*}
M_{N(t)}(s)
&= \E{e^{s N(t)}}
= \sum_{k=0}^\infty e^{s k} \P{N(t)=k}
= \sum_{k=0}^\infty e^{s k} \frac{(\lambda t)^k}{k!} e^{-\lambda t} \\
&= e^{-\lambda t} \sum_{k=0}^\infty \frac{(e^s \lambda t)^k}{k!}
= \exp(-\lambda t + e^s \lambda t) =\exp(\lambda t(e^s - 1)).
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:l-101}
 Use $M_{N(t)}$ to find $\E{N(t)}$ and $\V{N(t)}$.
\begin{hint}
Use~\cref{eq:69} and~\cref{eq:64}.
\end{hint}
\begin{solution}
Using the expression for the moment-generating function of~\cref{ex:53},
 \begin{equation*}
 M_{N(t)}'(s) = \lambda t e^s \exp(\lambda t(e^s - 1)).
 \end{equation*}
Hence $\E{N(t)} = M_{N(t)}'(0) = \lambda t $. Next,
$M_{N(t)}''(s) = (\lambda t e^s + (\lambda t e^s)^2) \exp(\lambda t(e^s - 1))$,
hence $\E{(N(t))^2} = M''(0) = \lambda t + (\lambda t)^2$, and thus,
$\V{N(t)} =\E{(N(t))^2}-(\E{N(t)})^2 = \lambda t + (\lambda t)^2 - (\lambda t)^2 = \lambda t$.
\end{solution}
\end{exercise}




\begin{exercise}\label{ex:1}
 Show with moment-generating functions that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
\begin{hint}
Dropping the dependence of $N$ on $t$ for the moment for notational convenience, consider the random variable
 \begin{equation*}
 Y = \sum_{i=1}^N Z_i,
 \end{equation*}
 with $N\sim P(\lambda)$ and $Z_i\sim B(p)$. Show that the moment-generating function of $Y$ is equal to the moment-generating
 function of a Poisson random variable with parameter $\lambda p$.
\end{hint}
\begin{solution}
Consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that $n$
arrivals occurred. Then we throw $n$ independent coins with success probability
$p$. It is clear that $Y$ is indeed a thinned Poisson random variable.

Model the coins as a generic Bernoulli distributed random variable
$Z$. We first need
\begin{equation*}
 \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p.
\end{equation*}
Suppose that $N=n$, then since the outcomes $Z_i$ of the coins are iid,
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n,
\end{equation*}
where we use~\cref{eq:73}.

With~\cref{eq:77},
\begin{align*}
 \E{e^{s Y}}
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}}
= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}}
= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}},\quad\text{by independence of $Z_i$ and $N$}, \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
= e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
&= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
\end{align*}
\end{solution}
\end{exercise}



\begin{exercise}\label{ex:41}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, use moment-generating functions to show that $N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$.
\begin{hint}
 Use~\cref{eq:73} and~\cref{eq:75}.
\end{hint}

\begin{solution}
\begin{align*}
M_{N_\lambda(t)+N_\mu(t)}(s)
&= M_{N_\lambda(t)}(s)\cdot M_{N_{\mu}(t)}(s)
=\exp(\lambda t (e^s -1))\cdot \exp(\mu t(e^s-1)) \\
&= \exp((\lambda + \mu)t (e^s-1)).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} \label{ex:96}
 Use moment-generating functions to prove~\cref{eq:bin}.
\begin{hint}
Solve~\cref{ex:53} and \cref{ex:1} first. Perhaps~\cref{ex:31} is also useful.
\end{hint}
\begin{solution}
Take $Y=\sum_{i=1}^n Z_i$ with $Z_i\sim B(p)$. Then,
\begin{equation*}
M_Y(s) = \E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n.
\end{equation*}
Recall that $p= \lambda t/ n$. Then, with~\cref{eq:65},
\begin{equation*}
\lim_{n\to\infty} \left(1 + \frac{\lambda t}{n} (e^s - 1)\right)^n = \exp({\lambda t (e^s-1)}).
\end{equation*}
\end{solution}
\end{exercise}



\input{trailer}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
