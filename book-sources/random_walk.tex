% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: clean: { extensions: [ aux, blg, idx, ilg, ind, log, out, pytxcode, rel, toc ] }
% !arara: clean: { files: [ ans.tex, hint.tex] }

\input{header}
\section{Queueing Processes as Regulated Random Walks}
\label{sec:queu-proc-as}


In this section, we provide an elegant construction of a queueing process based on  \emph{random walk}.
This serves two goals.
The fist is to show that queueing theory is essentially based on concepts of fundamental interest in probability theory (the random walk), hence is strongly related to many other applications of random walks, such as finance, inventory theory, and insurance theory.
The second is to show that it is (very) hard to characterize the transient behavior of a queueing process.
Thus, in the rest of the book we will only study queueing systems in steady-state, by which we mean that we consider the long-run average limits of the system.




\newthought{In the construction} of queueing processes as set out in~\cref{sec:constr-discr-time} we are given two sequences of iid rvs: the number of arrivals $\{a_k\}$  and the service capacities $\{c_k\}$ per period  which we use  in the recursion\sidenote{\cref{ex:24}} $L_k = [L_{k-1} +a_k- c_{k-1}]^+$ to
compute the number of jobs in the system.
By removing the $[\cdot]^{+}$ operator, we obtain a random walk $\{Z_k, k=0,1,\ldots\}$ with $Z_k$ given by
\begin{equation}\label{eq:44}
 Z_k = Z_{k-1} + a_k - c_k.
\end{equation}
To see that $\{Z_k\}$ is indeed a random walk, observe that $Z$ makes jumps of size $a_k-c_k, k=1,\ldots$, and $\{a_k-c_k\}$ is a sequence of iid rvs. Observe that $\{Z_k\}$ is `free', i.e., it can take positive and negative values, whereas $\{L_{k}\}$ is restricted to the non-negative integers.

It is interesting to express $\{\L_k\}$ in terms of $\{Z_k\}$. From~\cref{eq:44}, it is evident that $a_k - c_k = Z_k - Z_{k-1}$, hence,
\begin{equation*}
 L_k = [L_{k-1} +a_k- c_{k-1}]^+ = [L_{k-1} +Z_k- Z_{k-1}]^+.
\end{equation*}
It follows that  $L_k - Z_{k} = \max\{\L_{k-1} - Z_{k-1}, -Z_k\}$,
and, by completing the recursion,\sidenote{\cref{ex:l-133}} we obtain
\begin{equation}\label{eq:reich1}
 L_k = Z_k - \left(\min_{1\leq i \leq k} Z_i\right)\wedge 0,
\end{equation}
where $a\wedge b = \min\{a,b\}$.

This recursion for $L_k$ leads to nice graphs.
In~\cref{fig:random_bernoulli} we take $a_k \sim \Bern{0.3}$ and $c_k \sim \Bern{0.4}$.
\begin{marginfigure}
\begin{pycode}
import numpy as np
from scipy.stats import bernoulli
import matplotlib.pylab as plt
import tikzplotlib
from matplotlib import style
style.use('ggplot')

np.random.seed(3)

n = 30
a = bernoulli(0.3).rvs(n)
a[0] = 0
s = bernoulli(0.4).rvs(n)
z = a.cumsum() - s.cumsum()  # z is the random walk
zmin = np.minimum.accumulate(z)
q = z - zmin  # reflection

fig = plt.figure()
plt.plot(z, 'o-', markersize=2, label="Z")
plt.plot(q, 'o-', markersize=4, label='L')
plt.legend(loc='lower left')
plt.xlabel('Time')
plt.legend(loc='lower left')


print(tikzplotlib.get_tikz_code(axis_height='5.5cm', axis_width='6cm'))
\end{pycode}
\caption{An instance of~\cref{eq:44}.}
\label{fig:random_bernoulli}
\end{marginfigure}
We can clearly see that $L_k$ is equal to $Z_k$ minus the minimum of $Z_{i}, i\leq k$.


In~\cref{fig:random_walk}, $a_k\sim \Bern{0.49}$  the random walk behaves according to
\begin{equation}\label{eq:35}
 Z_k = Z_{k-1} + 2 a_k -1.
\end{equation}
Thus, if $a_k=1$, the random walk increases by one step, while if $a_k=0$, the random walk decreases by one step, so that $Z_k \neq Z_{k-1}$ always. Observe that this is slightly different from a random walk that satisfies~\cref{eq:44}; there, $Z_{k}=Z_{k-1}$, if $a_k=c_k$.

\begin{marginfigure}
\begin{pycode}
import numpy as np
from scipy.stats import bernoulli
import matplotlib.pylab as plt
import tikzplotlib
from matplotlib import style
style.use('ggplot')

np.random.seed(3)

# make a random walk with steps up and down.
# a is an array of bernoulli random variables
# z is the random walk, i.e., z[i] = sum_{j=1}^i a[i]
# zmin is the running minimum, i.e, \min{z(j), 0 \leq j \leq i}
# q is is random walk reflected in the origin, i.e,
# q[i] = z[i] - \min{z(j), 0 \leq j \leq i}

n = 30
a = 2*bernoulli(0.49).rvs(n)-1
a[0] = 0
z = a.cumsum()
zmin = np.minimum.accumulate(z)
q = z-zmin

fig = plt.figure()
plt.plot(z, 'o-', markersize=2, label="Z")
plt.plot(q, 'o-', markersize=4, label='L')

plt.xlabel('Time')
plt.legend(loc='lower left')
print(tikzplotlib.get_tikz_code(axis_height='5.5cm', axis_width='6cm'))
\end{pycode}
\caption{An instance of~\cref{eq:35}.}
\label{fig:random_walk}
\end{marginfigure}




\newthought{What can we} say about the transient (time-dependent) behavior of $\{\L_k\}$ with the above?
To obtain insight into this question, let us suppose that $a_k\sim \Pois{\lambda}$ and $c_k \sim \Pois{\mu}$.
Then, by~\cref{ex:l-103},
\begin{equation*}
 Z_k = Z_0+N_{\lambda, k} - N_{\mu, k}.
\end{equation*}
With a bit of work, we can show\sidenote{\cref{ex:l-134}} that\sidenote{Note that the difference of two Poisson random variables is not Poisson, in contrast to the sum.}
\begin{equation}\label{eq:rw2}
 \P{Z_k=n}
= e^{-(\lambda+\mu)k} (\lambda k)^{n-m} \sum_{j=0}^\infty
\frac{(\lambda\mu k^2)^j} {j!(n-m+j)!}.
\end{equation}
It is apparent that only formally the distribution of $L_{k}$ can be obtained from this and~\cref{eq:reich1}, but  we will not obtain a simple  expression. Consequently, we cannot find useful expressions for $\P{L_k=n}$ as a function of $n$.\sidenote{We refer to $k\to\P{L_{k}=n}$ as  the transient behavior of $\{L_{k}\}$.}

Now observe that $Z_k$ is just the free $M/M/1$ queue, which is just about the simplest queueing system to analyze; other queueing systems are (much) more complicated. As we already have to give up our attempts to analyze the transient $M/M/1$ queue, we have to give up our hope, but contend ourselves henceforth with the analysis of queueing systems in the limit as $t\to\infty$.


\begin{exercise}\label{ex:l-133}
Show that $L_k$ satisfies~\cref{eq:reich1}.
%the relation $L_k = Z_k - \min_{1\leq i \leq k} Z_i\wedge 0$.
\begin{hint}
  Use recursion and use subsequently,
$\max\{\max\{a,b\}, c\} = \max\{a,b,c\}$, $L_0 = Z_0$, $\max\{-a, -b \} = -\min\{a,b\}$.
\end{hint}
\begin{solution}
From the recursion and the hints,
\begin{equation*}
 \begin{split}
 L_k - Z_{k}
% &= \max\{\L_{k-1} - Z_{k-1}, -Z_k\} \\
&= \max\{\max\{\L_{k-2} - Z_{k-2}, -Z_{k-1}\}, -Z_k\} \\
&= \max\{\L_{k-2} - Z_{k-2}, -Z_{k-1}, -Z_k\} \\
&= \max\{\L_{0} - Z_{0}, -Z_1, \ldots, -Z_k\} \\
&= \max\{0, -Z_1, \ldots, -Z_k\} \\
&= - \min\{0, Z_1, \ldots, Z_k\}.
 \end{split}
 \end{equation*}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:l-134}
 Show that $\P{Z_k=n}$ satisfies~\cref{eq:rw2}.
% \begin{equation*}
%  \P{Z_k=n}
% = e^{-(\lambda+\mu)k} (\lambda k)^{n-m} \sum_{j=0}^\infty
% \frac{(\lambda\mu k^2)^j} {j!(n-m+j)!}.
% \end{equation*}
\begin{solution}
\begin{align*}
 \P{Z_k=n}
&= \P{m+N_{\lambda k } - N_{\mu k} = n }
= \P{N_{\lambda k} = n - m + N_{\mu k}} \\
&= \sum_{j=0}^\infty \P{N_{\lambda k}  =  n - m + j, N_{\mu k} =j}
= \sum_{j=0}^\infty e^{-\lambda k} \frac{(\lambda k )^{n - m + j}}{(n-m+j)!} e^{-\mu k} \frac{(\mu k )^j}{j!} \\
&= e^{-(\lambda+\mu)k} (\lambda k)^{n-m} \sum_{j=0}^\infty  \frac{(\lambda\mu k^2)^j} {j!(n-m+j)!}.
\end{align*}
\end{solution}
\end{exercise}




\begin{exercise}\label{ex:l-147}
 Suppose\marginpar{When we start with a really large queue, we characterize the time to clear the system in~\cref{sec:n-policies}.}
 for the $G/G/1$ that a job sees $n$ jobs in the system upon arrival.
 Use the central limit theorem to estimate the distribution of the waiting time in queue for this job.
\begin{hint}
 Let $W_{n} = \sum_{k=1}^n S_k$.
 Since $\{S_k\}$ are assumed to be iid\ for the $G/G/1$ queue, $W_{n}$ has mean $\mu_n = n \E S$ and $\sigma_n^2 = n\V S$.
\end{hint}
\begin{solution} Under conditions you can find on the internet, $(W_{n} - \mu_n)/\sigma_n \to \mathcal{N}(0,1)$ as $n\to \infty$,
 where $\mathcal{N}(0,1)$ is a normally distributed random variable
 with $\mu=0$ and $\sigma^2=1$. But then
 \begin{align*}
 \frac{\W_{n} - \mu_n}{\sigma_n} &\approx \mathcal{N}(0,1) \iff  W_{n} - \mu_n \approx \sigma_n \mathcal{N}(0,1) \iff
 W_{n} - \mu_n \approx \mathcal{N}(0,\sigma_n^2) \iff \\
 W_{n} &\approx \mu_n + \mathcal{N}(0,\sigma_n^2) \iff
 W_{n} \approx \mathcal{N}(\mu_n,\sigma_n^2) = \mathcal{N}(n\E S, n \V S),
 \end{align*}
% where we used in the last equation the fact that the variance of a (fixed) sum of iid random variables is equal to the sum of the variances.
\end{solution}
\end{exercise}


\input{trailer}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
